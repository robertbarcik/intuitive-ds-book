# Uncategorized topics {#uncategorized-topics}

This chapter contains a list of topics which I consider interesting but I am unable to currently categorize them anywhere.

Spark
Bayesian methods
Text processing
Topic modelling
Shell and Bash?

Be Curious
You are already at the level where curiosity will be what keeps you growing. With this chapter we are starting with a new practice that I will be leaving recommendations for additional sources that you can read 

Is Machine Learning Unethical or Racist?

Are data scientists evil? Is there a reason why to be scared of these stalkers?

Propensity models
Can two models and their respective probabilities be really compared?

Customer and user
You need to distinguish between the two

The falacy of absolute numbers
For many years what matters a lot is a conversion rate or a success rate. I recommend though to be careful with it.

Should I focus on how my code looks?
I have a saying: "People have feelings. They feel when you code like sh_t."

Long and wide format of data

A journalist asks a programmer: What makes a code bad?
Programmer: No comment

Parallel programming

Self-organizing maps

*How does my PC view features?*
Numerical and categorical features. This needs to come before Supervised Learning as I am explaning classification and regression. No it doesn't, it is explained only conceptually.

```{r machine learning, echo = FALSE, fig.cap='Posted by u/victor_stefan to r/ProgrammerHumor, Reddit.'}
knitr::include_graphics("resources/02-observer/machine-learning.png")
```

*What is information leakage?*
One of the steps we need to ensure is to prevent *information leakage*. This happens in cases, where our independent features contain some *leaked* (or spoiler) information about our target feature. Let me give you an example. 


*What is correlation and causality?*

Make an example with babies and birds.


*Jobs which might talk about you*

I would say that positions mentioning statistics, causal inference, experimentation or A/B testing, incrementality measurement, segmentation, decision science, economics, and "analytics" generally will probably be more of that flavor. Emphasis on communication skills and visualization will be a component to these roles. You probably want to avoid applying to positions with a focus on engineering, architecture, or those specifically calling out deep learning or computer vision applications.

I'd say a solid 50 percent of DS jobs want an insight specialist over a ML specialist regardless of the job posting.

*Skillset wrong view*

Machine Learning Areas:
Supervised Machine Learning (49%)
Unsupervised Learning (26%)
Time Series (25%)
Natural Language Processing (19%)
Outlier detection (16%)
Computer Vision (15%)
Recommendation Engines (14%)
Survival Analysis (8%)
Reinforcement Learning (6%)
Adversarial Learning (4%)

Machine Learning Techniques:
Logistic Regression (54%)
Decision Trees - Random Forests (43%)
Support Vector Machines (32%)
Decision Trees - Gradient Boosted Machines (31%)
Bayesian Techniques (27%)
Neural Networks - CNNs (26%)
Ensemble Methods (22%)
Gradient Boosting (17%)
Neural Networks - RNNs (15%)
Hidden Markov Models HMMs (9%)


*Current hiring*
I saw an article somewhere the other day that suggested that even really good Machine Learning candidates were failing to get good jobs.

Apparently the universities are now churning out many ML graduates ... but the jobs are surprisingly not there.

The article suggested that the top users of AI have decided to build very small teams of stellar AI gurus : geniuses with multiple PhDs. The firms do NOT want hoards of simply 'very good' ML candidates.

Is this likely to be true?

If so, is it just the top firms which are this fussy? In which case, are there other firms who are happy to hire the 'good' ML candidates?


*What is so exciting about this profession?*
The endlessness of the game is my favourite part.

I like always having new things to learn. Hard to imagine ever becoming bored.

*More on definition of the field*
correct in the fact that analytics has been around a long time. Only recently has it graduated into the realm of BI.

I'll preface everything here with this: EVERY industry/company/department has slightly different needs and scope so YMMV.


Within BI there are three main disciplines: Data Engineer, Data Analyst, Data Scientist.

Data Engineers focus on the data at point of capture, storage architecture, and access. This involves multiple parties too if the enterprise is large/complex enough; DB architects, DB Admins, Security and so on. In essence the DE focuses on making the data accessible down the data pipeline.

Data Analyst uses both prescriptive analytics, the use of historical data to detail where you and and where you've been, and predictive analytics, the use of that same historical data to look for how changes in business can effect whatever business KPI/SLA/Metric they are trying to improve.

Data Sciencist uses that same historical data but in a more theory approach. They commonly are looking for new and unforeseen instances. They also look more at academic approaches; automation of predictive models to allow for streamlined processes (see digital twinning in manufacturing), use of AI/ML to limit need for human intervention (see neural nets deployed as "Brokers" in insurance), or NLP/sentiment analysis (see Alexa/OK Google or websites like booking.com)

A journeyman DS is not so different from a DA. Both are using the tools and techniques at their disposal to drive data driven decisions within their company/organization. One may rely more on BI tools like Alteryx/Tableau, the other may focus more on statistical programming like Python and R. In the end they are all part of the same ecosystem. A DA may focus more on the business processes (a SME for the business) while a DS may look at things more clinically.

To get back to your question on what "pure" data scientist does? That is harder to quantify. There are Data Scientists that focus purely on model development. Only looking at statistical data sets to derive better/fast models in ML/AI. They hardly ever work on real world material and drive a business decision. They are also like any other theory driven scientist; a very small sub-class of the whole.

*Interview questions*
https://old.reddit.com/r/datascience/comments/edhzdg/d_data_science_interview_questions_mega_thread/

*What is right and wrong*
Thinking back to my days as a first year data scientist, one of the most difficult transitions I've seen people make is how they measure their value.

Because academia is primarily an environment in which you're measured by how right or wrong you are, a lot of people transition into the workplace thinking the same. What's worse, some go further and extend that to the point of thinking that there is value in proving others wrong.

That is fundamentally not going to work. And that is because people in the workplace are measured almost exclusively on how productive they are - they are measured on results.

Corollary 1: if it's wrong but it works, then it's not wrong.

Corollary 2: if you're right but it doesn't change the outcome, then it doesn't matter.

Corollary 3: if you're right, but it doesn't work, then you're wrong.

Corollary 4: if you prove someone else wrong, but their answer works and yours doesn't, then they're right and you're wrong.

Corollary 5: if you prove someone's solution to be wrong even though it does provide value, then you have not yet provided any value until you propose something better.

I cannot emphasize how much you can limit your career by focusing on right vs. wrong. Right vs. wrong is irrelevant; productivity always rules.

EDIT: Since many have had an issue with the definition of something that works vs. something that is wrong:

This is the part that people miss - it is rare that bad science works.

When things that a person sees as "wrong science" work, I normally find that the overwhelming majority of the time, if that person is junior, what is actually happening is that:

It's not actually wrong, and the person just doesn't understand why it's right.

It's not 100% right, but it's right enough to provide value. And some people interpret that to mean wrong, which is too binary in the world of modeling. 95% right isn't wrong, it's just 95% right.

The only scenario where you will see bad science work with any degree of frequency is when it has been tested over too limited a set of scenarios - in which case it should be relatively easy to point out where it will fail, and then you can focus on outputs - on how it won't work, rather than on it being wrong.

Excellent points! I'll add that fancy novel techniques mean almost nothing to business folks. Simple and effective is just as good if not better. You can and will get out played by someone with a simple brute force technique that they implement quickly while you try and perfect yours.

*More on citizen data scientist*
Although the term has existed for a couple of years now you won't find job listings for citizen data scientist on Glassdoor.com. That's because it's not a role that an organization is going to hire for, but more like a requirement, they need to fill. People in the industry are using the term citizen, but those doing the hiring are focused on the tasks not currently getting done but require attention. 

It's common knowledge that we have a shortage of data scientists, and businesses realize that not every job function needs to be done by someone with an advanced degree as they reckon with this shortage. Instead, a citizen data scientist can be trained to do work that is needed but not currently getting done. 

*Develop visualisation skills as a crucial skill*
Once you've got the foundation of big data skills, you need to develop visualization skills. Your employer or organization doesn't care about a jumble of numbers on his or her desk. Your job is to present the data in a visually compelling way.

Google Data Studio as a learning alternative to Power BI, Tableau or Clicksense

*Negative opinion*
What will not work is creating a partly automated software and expect people with little training or understanding of data to be able to use it in new situations. Imagine a combination of untrained pilot and an auto-pilot that works 90% of the time. The plane will fly fine for a while until an unpredictable situation will arise, and the autopilot will signal a problem and revert control to the untrained pilot. Bad things will happen. Similar logic is behind Google decision to remove a steering wheel from its self-driving cars - a driver who is relying on a car almost all the time is unlikely to react quickly and adequately in an emergency when the car cannot cope.

I should state this at the beginning of the book that I do not intend readers to become partly trained. 

*Should I rename the term?*
I run a small team of data analysts and I'm offended by the term "citizen data scientist" to describe us. We grew from the business developing data skills as we went along. So, we don't have much formal training, but we can run circles around other analysts with all the proper credentials. Calling us citizen data scientists sounds very condescending. Like we're still sitting at the kid's table. Keep the terms separate. I acknowledge that we're not "data scientists" by any formal definition, but we are skilled data analysts. Someday we may earn the coveted title of, "Data Scientist, " but until then don't put me at the kid's table.

*Steps in ML project*
https://www.lpsm.paris/pageperso/has/source/Hand-on-ML.pdf
This is what Oreilly says, which I tend to disagree with.
1. Look at the big picture.
2. Get the data.
3. Discover and visualize the data to gain insights.
4. Prepare the data for Machine Learning algorithms.
5. Select a model and train it.
6. Fine-tune your model.
7. Present your solution.
8. Launch, monitor, and maintain your system

I like how they showcase what a confusion matrix is.
They also describe ROC curve
Also Precision Recall tradeoff

Linear regression is a copy from Andrew NG

*Personality of Data Scientist*
Get into flow
