[
["index.html", "Intuitive Data Science Chapter 1 Foreword 1.1 Current state of this book 1.2 Development and To-do", " Intuitive Data Science Robert Barcik 2020-03-16 Chapter 1 Foreword Data Science is undoubtedly one of the fastest growing areas within IT. This growth is not unjustified though. Majority of the firms did undergo a process of digitalization within the past 2-3 decades. Opportunities within digitalization, such as digital offering of products, digital channels for marketing, are reaching new maturity points. If we look around ourselves, majority of products which make sense to be digitalized, indeed are (if we disregard laggard governments). According to microeconomics and its law of diminishing marginal utility, for every penny invested early into an activity, the benefit on this penny will be large. While if we keep investing, after certain time the investment of same penny will yield only much smaller benefit. Due to this reason, lots of companies are these days turning from digitalization (as a sole investment) into a related field of Data Science. Figure 1.1: Display of Law of Diminishing Marginal Utility. The utility obtained from eating every next chocolate bar yields less utility, until the utility even starts to be negative. This is supported by the fact that the process of digitalization inevitably collects data. Thereafter companies see that even with minimal investment into this trend, the benefits can be significant. Information and data are some of primary growth drivers of current decades. And this is where you come in, with your opportunity to grow your career, naturally following what market is hungry for. If you have made (maybe even now) the big decision to join Data Science, you will ask what should you learn and how. I have seen many people discouraged during this path of Data Science learning, which was really saddening for me as I personally really enjoyed this path. It was bothering my mind until the point that I decided to write this book, to address what is currently lacking in the market: We as Data Scientists are poor teachers. There is a vast amount of books for new learners, but the reader will fall asleep more times than not while reading. There are numerous platforms and online courses, but these teach skills instead of highlighting clear path that a student should follow. We are asking newcomers to learn what we were taught - endless statistics, programming, lengthy math formulas. Is it really necessary though? The world of Data Science has changed and now there is a need for a new generation of Data Scientists to arise. Both of these issues which I saw encouraged me to write this book. Already in the next chapter I will give you (hopefully) a fresh view on the path which you might take to become a Data Scientist. Secondly, I will teach you only the required concepts and rather encourage you to use your domain knowledge, may it be anything from Banking, through Medicine, up to Engineering - to apply successful Data Science. Finally, you will see that I give up all math formulas, lot of programming and try to explain you things in an intuitive way. Enjoy the read, Robert 1.1 Current state of this book Chapters where first draft is done and are ready for editing: Foreword. Being Aware. Observer. Chapters currently in development: Contributor Part II Statistician Chapters where only basic framework has been outlined: Contributor Part I Contributor Part III Project Responsible Butter Knife 1.2 Development and To-do Add examples in “Anyone Can Become Citizen Data Scientist” Reconsider How does Data Science project look like? "],
["being-aware.html", "Chapter 2 Being Aware 2.1 Five Cornerstones 2.2 Defining Data Science 2.3 Defining Data Scientist 2.4 The Struggle of Data Science 2.5 Who is Citizen Data Scientist 2.6 Value of Citizen Data Scientist 2.7 Anyone Can Become Citizen Data Scientist 2.8 The 6 Levels of Citizen Data Scientist 2.9 Real World Experience 2.10 Learning Without Math 2.11 Understand a Population 2.12 Mode, Median and Mean", " Chapter 2 Being Aware 2.1 Five Cornerstones You will quickly notice that this book is structured quite differently compared to other offerings on the market. Let me reason about that, even before I introduce you to the structure and the transformation that this book offers. I have been a lecturer/trainer for a few years and I always notice that content does not matter as much as the path through the content as well as the assumptions upon which the content is built. Only if both of these are fine, the book/course/training can offer you (the reader/student) a transformation - taking you from where you are, to where you want to be. In this case I am hoping you would like to make a living within Data Science or create some amazing project within it! This book is hence built on five cornerstones: Data Science is a practice of using data to create a direct benefit. Citizen Data Scientist is becoming a crucial occupation. Anyone can become a Citizen Data Scientist. There are 6 levels of Citizen Data Scientist and these should act as a learning curve. Math is not required as a teaching tool in order to train a Citizen Data Scientist and can be replaced by visualisation and intuition. Did I go crazy claiming that anyone can become a Data Scientist? And what about the Math which is appraised by everyone as the crucial tool within data science?!? Maybe I have, but these cornerstones (assumptions) worked for me as well as for my students within the past years. Let me in this chapter explain why I believe that these things might work out for you. 2.2 Defining Data Science The very first thing that I would like to do is to answer a simple question: What is data and what is information? The difference between the two is in their value. Data inherently do not have any value for their owner, simply because he/she cannot do anything beneficial upon them. On the other hand, information can be valuable for its owner. These two are however thinnly connected - as information is derived from data, while this process is called Data Science. I will give you an example from my recent past on how I got a value out of Data Science. I was always interested in my sleep, as I know that in order to live a happy life, I shall sleep well. The only two things I know really is how long I sleep and how do I feel in the morning. I then found an app on my phone which was promising to analyze my sleep and give me recommendations on how to improve my sleep. The app is using accelerometer in my phone, recording my movements at night. During the first two nights, the app only collected data - every morning I was able to see various graphs on how I slept. After a week though, recommendations came such as “when you go to bed late, around 23:00, even if you sleep full 8 hours, your sleep quality suffers.”. This is already information which had a value for me. The app is Data Science process of creating valuable informaton from data. If you now turn to search engines and start to search for Data Science terms, you will be overwhelmed by articles and (unfortunately) buzzwords - Machine Learning, Data Preprocessing, Artificiall Intelligence, Tensorflow, R, Python. These are at the end only methods and tools of Data Science to achieve a result which we described above. Do not get confused or overwhelmed by these. The book which you are reading is a way how to master these methods, but first I want you to truly and depply understand the cruial things and ideas of Data Science. This might actually be the most important message from this book (that is why it is in the beginning) - always keep in mind that the methods should always serve the purpose which Data Science has. Data Science is merely an art of turning data into information, which provides benefit. As you will inherently meet all of these weird terms, let’s go through them as it can be part of defining what a Data Science is. Let’s first start with the three major terms, Data Science, Machine Learning and Artificial Intelligence. Here is one think that you need to know about them: Data Science != Data Engineering != Machine Learning != Artificial Intelligence This is the first learning which you might take from this book - all four of these terms mean something different, yet they are connected. As we already know from previous paragraphs what Data Science is, how does Data Engineering connect to it? Quite simply, by making the data available and creation of infrastructure where Data Science can happen. Without Data Engineering, it would be then tremendously hard to do Data Science, as you would not have access to the data which you would like to have and you wouldn’t have sufficient infrastructure, such as powerful servers, for analyzing this data. Machine Learning is simply a set of practices and tools to build what we call Machine Learning models. Actually, you are going to learn a lot about Machine Learning, as this is a great possibility how one can derive useful information out of data. Machine Learning is then what connects Data Science to Artificial Intelligence. This last field might be probably the most complex as it not only works with data, information and machine learning models; but also with automation, robotics and creation of intelligent systems. Again though as with Data Science, Machine Learning is an integral part to it; and as such important cornerstone to two fields is getting a lot of well-deserved attention. 2.3 Defining Data Scientist Now that we know what Data Science is, it is quite easy to define who a Data Scientist is. He/She is a practitioner of this field, who uses its methods to achieve its goal. Usually, Data Scientist has general education in three area - Programming, Mathematics and Statistics. Notice that none of these areas deal with context or domain such as Banking, Medicine or Engineering. Thus Data Scientist - by definition, should be domain-agnostic. We can see this also in the definition within Oxford Dictionary: a person employed to analyse and interpret complex digital data, such as the usage statistics of a website, especially in order to assist a business in its decision-making He/she should be able to join any of mentioned domains, and many more, apply the methods of Data Science to fulfill the goal of creating value, through information, out of data. This is very important point for you as you will now see. 2.4 The Struggle of Data Science Now that you heard about Data Scienstits, who are Citizen Data Scientists? I think they are the most crucial occupations in the future of the field of Data Science. You heard me right, not the original Data Scientists, but the new generation of Citizen Data Scientists. Let me show you why. Data Science as a field has been around for decades in one form or another. There has been market analysts, risk modellers, product developers, artificial intelligence developers and so on. If we however relate to the definition of Data Science above in a way that a Data Scientist can essentially come to any company and make use of its data to create benefit, we can go back to the millenia breakthrough. Universities start to offer programs with names like “Data Analysis”, “Business Intelligence” and first candidates are leaving these programs. It feels like the field is growing tremedously - growth in GPU powers, Deep Learning, Automated Machine Learning products. Especially since 2010s, job advertisments revolving around Data Science are exploding! Though is the field really growing from the perspective of market? I would say very slowly, simply because the created benefit for companies (and their customers) is growing slowly. This is caused by the fact that the field is still relying on the slow and linear growth of the first generation of Data Scientists - ones having formal education, being able to guarantee the work. Now two scenarios can occur when a company attempts to create benefit/profit through data science: It hires actual Data Scientists, whereas costs to hire them are tremendous and what is even more expensive is to make them productive. Finding for them process which can be optimized through their methods and then properly optimizing it is lengthy and costly. As these are domain-agnostic, the company needs to also educate them and integrate them into contexts. Company slowly looses interest and stops believing that actual profit in a decent scale can be created this way. It does not hire actual Data Scientists, as they are expensive and scarce. People who come to the company have been originally trained in other areas and are now expected to be actual Data Scientists. The company has just thrown away a lot of their potential as only their recently acquired experience are counted and not their original context. The hirees are now pushed equally hard as the actual Data Scientists and are struggling to create valuable products. Company slowly looses interest and stops believing that actual profit in decent scale can be created this way. You might now disagree with me and say I am too much of a negativist. To justify my thinking in your mind, let me ask you only one question. As employees, we are expected to create larger profit than what we cost our employer. How many data scientists you know, who can prove that they are repeatedly creating profit for their companies? Not so many, right? Thereafter I believe that above described struggle holds. 2.5 Who is Citizen Data Scientist I believe that the answer to this struggle of many companies is recognition of a Citizen Data Scientist (the ones coming in point 2 above) and definition of their relationship to Data Scientists. The market is begging us to do so and we can see it by who comes to job interviews when we make an advertisment for a position of a Data Scientist. These are people who do not have formal education or direct experience. Let me tell you one story from my past. I worked in large organization with thousands of employees as a Data Scientist. Out of nowhere, I received an email from a colleague I never heard of before. She claimed that she is really passionate about the methods of predictive modelling and Data Science and is eager to apply these methods in her CRM department. We met and she showed me R-script in which she tried a bunch of Machine Learning algorithms and preprocessing methods. What I did in turn was that I encouraged her to continue with the passion because the field has a future and I advised her about further methods she can try. Only years later, I realized my mistakes in that meeting and that I met one of the first Citizen Data Scientists. I then learned that she left the company as she never managed to get her passion to productive work. I was one of the reasons why she churned because I gave her advice on how to become the first generation Data Scientist, not the second generation of Citizen Data Scientist. Let’s look at the definition by Gartner in 2016: Citizen data science bridges the gap between mainstream self-service data discovery by business users and the advanced analytics techniques of data scientists. This colleague of mine was originally in the first part of definition (business user), while I was in the second part of it (advanced analytics). She was trying to become a bridge between the two, without even realising it! Unfortunately I am only smarter years later and when the same situation happens these days, I react in a different way. Also, the company which I work for now is smarter and allows for an environment where Citizen Data Scientists are given space. She had something that I could never have - expertise in her CRM department and if I was about to give her advice now it would be: Focus on how a predictive model will create benefit in your department. Unless you create a benefit, whatever we code is worthless. Focus on applying in your project your domain knowledge. Only you know it, I don’t. Here is a (short) list of things you should try in your R-script (most of which you already have). You do not need to push for more complex methods and crazy good programming, what you have is already enough. If something more complex is required later on, I will do it for you. Ehm, exact opposite of what I originally did, right? Citizen Data Scientist therefore stays in his/her original deparment and context, and only to a required (limited) extend enhances her knowledge on Data Science, so that he/she can create the value. He/she posesses edge over (first generation) of Data Scientist in the context knowledge and can hence create more valuable products. 2.6 Value of Citizen Data Scientist As everyone says it these days - data are everywhere, companies just need to start utilizing them. We can thus take as granted that Data Science will have justification in upcoming years. Though why do companies need Citizen Data Scientists and why they should focus on them instead of the first generation of Data Scientists? Simply because the growth in created benefit will be much greater through Possibility of having higher counts of Citizen Data Scientists, as compared to Data Scientists due to the fact that extensive formal education is not required. Possibility of having more impactful Data Science projects as these will be integrated better to the business, if created by properly trained Citizen Data Scientists. Citizen Data Scientists do know better what needs to be optimized and in which way, because the context (business area) is their origin. Is it necessary then that every Citizen Data Scientist is on the same level of knowledge? Of course not, that is why I will be basing this book on 6 levels of Citizen Data Scientist, which should only achieve a level suitable for his/her involvement to Data Science. So, do you want to become one of them? What happens with the original (first generation) Data Scientists? We will still need them, just in slightly different role as until now. They become heavy developers, guarantors and trainers. Whenever a complex problem occurs, which needs technically challenging solution, it will be them who focus on it. The thing is, that in many organizations, there is only a handful of such problems, that is why this makes sense. Secondly, they act as guarantors of solutions developed by Citizen Data Scientists. This again makes sense, because reading through something and commenting on it takes a fraction of time as compared to developing it. Finally, they should act as trainers as there is a need to train, coach and mentor a lot of Citizen Data Scientists. 2.7 Anyone Can Become Citizen Data Scientist I honestly believe that whomever you are, you can become a Citizen Data Scientist. Why? Because Data Science methods add up to only about half of the requirements for a success, the other half is context knowledge and application. Thereafter whatever your background is, you can become one and create value to your company. Moreover, there is no single-level of Citizen Data Scientist in my mind, but several of them. 2.8 The 6 Levels of Citizen Data Scientist There are many great courses, programs or coding platforms available online - why should you then read this book in particular? It is because I plan to help you grow through capabilities, not through skills. It is actually one of the reasons why I decided to write this book. I see often times learning content structured through skills - Importing Data, Cleaning Data, Preprocessing Data, Basics of Machine Learning; being the most common path out there, sometimes being up to 80 hours long. The issue is that you will most likely be only able to apply yourself once you learned all the skills. That is not good, as Data Science is a lot about practice and connecting to context where you want to apply yourself as Citizen Data Scientist. That is why this book sets 6 levels through which you will be growing. With each new level, your capabilities and powers within Data Science will inherently grow and so will your value for projects and employers. This gives you the ultimate opportunity to practice your skills - because you will be able to; in real world! As you will notice, each level holds its definition and a length required (from my perspective) to reach this level from a previous one. Level 1: Being Aware You know what Data Science is about, what it can and cannot achieve. You also know what path lies ahead of you as a Citizen Data Scientist. Learning length: 1 hour Level 2: Observer You are able to observe a Data Science project running, without being able to contribute to it. You are though able to make use of the outputs of a Data Science project as well as help with inputs for such project. If someone presents you a Data Science project, you are able to question its qualities. Learning length: 5 hours Level 3: Contributor You are able to assume simple task(s) within Data Science project, and create a benefit to the project productively - Data Preprocessing, Data Visualisation and Baseline Machine Learning. Learning: 30 hours Level 4: Statistician You are able to assume any basic task within a Data Science project and hold responsibility for statistical parts. Learning: 40 hours Level 5: Project Responsible You are able to define Data Science project from scratch and execute it. To do this, you are also able to organize a team around a Data Science project and work with peripherral fields such as Data Engineering. Learning: 40 hours Level 6: Leader You are able to grow Data Science initiative in your organization both effectively (hiring) as well as conceptually (specializations). Learning: 40 hours Each of these levels is very different both from the content perspective, as well as from desirable approach by you. For example, on a Statistician level, you are going to be focused solely on statistical formulas and their intuition. Be prepared for sitting longer hours, stretching your analytical mind. All of a sudden, this stops and in order to become a Project Responsible, we are going to sharpen your soft skills, such as how to organize a team effectively. It will be needed to free your mind and think about people instead of code. This book unfortunately cannot cover all that you should know on each of these levels. What the book intends is to give you all intuition and awareness of concepts which is required on each level. The reason why I decided to write this way (not covering concepts through technical details and coding), is that I believe from my experience that once the intuition in the concept is achieved, then it will become rather simple for you to apply it. 2.9 Real World Experience What is the biggest struggle of anyone who would like to become a (Citizen) Data Scientist? To get real world experience and practice. Whatever people claim within their online courses and trainings, the trainer will never be able to offer you what is awaiting for you in the real world of Data Science. Even I don’t claim it about my trainings. The only truthful way is hence to go to a real project. Use the 6 levels of Citizen Data Scientist exactly for this purpose! Let me show you an example of why I decided to write and teach in this way. You come to a job interview (external or internal one) and say that you have been learning and would like to become part of this Data Science project. The conversation might go as follows: Interviewer: Great to see your interest. So how can you contribute to our projects and help out? You: Well, I learned a bit of Python, I am able to put together some basic statistical model and also do some data preprocessing. Let me stop here for a second and explain to you what is happening behind this scene of this situation, which is happening probably hundreds of times every day. You are having hard time selling yourself and interviewer is having hard time evaluating your qualities. You are both stuck, due to simple reason - you have listed skills instead of capability to contribute. Now let’s do a similar conversation and we will reuse one of the levels from above as your answer: Interviewer: Great to see your interest. So how can you contribute to our projects and help out? You: I am able to act as an “Observer” to your Data Science projects. This means that I cannot directly contribute to it’s productive code pipeline, but I am able to help out with inputs and outputs. Due to my extensive background in Retail Banking, I know what features might be interesting to collect about customers and used in models and also how to apply outputs of your project in Marketing Campaigns. When a Data Scientist will put together a Machine Learning model, I will be able to do basic evalution of its qualities. Do you feel the difference between the two conversations? Instead of talking about skills, you started to talk about a possible contribution which you can do. You also very clearly drew a line of what you cannot do and the Interviewer will have it easier evaluating you. Moreover, you had a space to relate to your previous field - we all have something where we are coming from. This is what really matters for Citizen Data Scientist - show possible contribution that will be impacting the project and benefit out of it directly. 2.10 Learning Without Math Mathematics is undoubtedly the foundation of many fields, Data Science being one of them. If one masters it, incredible beauties will uncover in a lot of mathematical formulae and deep understanding of many concepts can be achieved. It has only one problem…“it’s f_____g hard to learn!!!”. I personally know only two kinds of people. First group are ones who are comfortable with math - they liked the subject since many years and have the incredible patience to learn it. Most of the times they also invested into some form of formal education within math. The second group are people who don’t like it and whenever they meet it, they search for intuitions and workarounds just to get the piece of work done. The share of people who belong to the latter group among my friends is 95% and that could be a sad fact for a field like Data Science which would like to grow, while being based heavily on Math. Or is it? In my perspective, the first generation of Data Scientists are primarily the ones who are fine with map and the second generation belongs to the non-likers. Now let me tell you, it’s perfectly fine and natural to not like math and not be comfortable with it. Who should adjust? Should it be 95% of population or teaching methods of Data Science? Let me bet on the latter… That is why this book will not teach anything through mathematics. I honestly believe that in order to train a Citizen Data Scientist, math is not only un-needed but also un-recommended, based on my argumentation above. P.S. There will be a few statistical concepts, so don’t blame me if they look like math. As I promised though, there will be no formula, just intuition. 2.11 Understand a Population In order for Data Science to fulfill its purpose of turning data into information, it is often times necessary to understand a population. In fact, if you are able to only describe a population, you already have tremendous powers for Data Science, called descriptive statistics. We can start by defining a population; which is in essence a synonymn to a set of individual points. Population usually refers to living organisms such as people, animals or other living organisms. But as these living organisms are from statistical point of view just sets of individual points it can be anything else - fleet of buses, set of marketing campaigns or batch of machineary. Now, why would we want to describe a population? Simply, because we want to understand it and know some information about it. Having in front of you an entire list of goals scored in the last hockey season will not help you determine who was the best player of that season by itself. You will need to search for which players scored the most goals, maybe look even deeper. If someone would show you a weather for the past two years, broken down to individual days, it wouldn’t be exactly easy to estimate what will be the weather tomorrow. We need to somehow aggregate, summarize or describe these populations in order to determine something meaningful. 2.12 Mode, Median and Mean We can start with the simplest tools of descriptive statistics - measures of central tendency. Don’t get scared, the phrase just stands for describing what lies in the middle of the population which we aim to describe. You most likely already applied these tools, by referring to an average. Maybe, sometime in the past you were asked some of these questions: What is usually the weight of loaf of bread sold in shops? (average weight of bread) What is usually the temperature in your city in summer months? (average temperature in summer) What is usually a salary in your country? (average salary in a country) Most likely you answered these questions by intuition in your mind. If you would however turn to the world of descriptive statistics, there would be three basic tools available for you to answer them - mode, median and mean. Mode is the simplest measure of average of the three. In order to calculate mode, you simply need to look at the most common occurence in your population. This could be a good measure for answering the bread question. We could walk into the shop and observe the weights of loafs offered there such as follows: loafs_weights_grams &lt;- c(500, 500, 400, 1000, 500, 250, 500, 250) Just by looking at the weights, we can easily determine what the mode is - 500 grams as this is the most common value. We can verify it by using an R function for calculating mode. getMode(loafs_weights_grams) ## [1] 500 If you think about the question intuitively, does mode provide us with a satisfactory answer? Most likely yes - usually the loafs weight 500 grams. Unfortunately, mode is not always the best representation of average. We can see it’s incapabilities if we try to answer our second question regarding average temperature in the summer. The data which we have about summer temperatures are as follows: summer_temperatures_celsius &lt;- c(35, 35, 32, 28, 31, 25, 26, 27, 24) If we used mode, and picked temperature which is occuring the most times, or in other words is the most common, this would be temperature 35. If we think about it intuitively, is it the best answer? Certainly not! The average temperature by our intuition and looking at the numbers is somewhere below of 30. Luckily we have another tool from descriptive statistics at our disposal - median. Median is also a rather simple tool, as it simply finds the middle observation. The way how we do that is rather straighforward; we sort the observations from smallest to largest and take the one in the middle. Let’s use R function to sort the temperatures and visually find the middle value. sort(summer_temperatures_celsius) ## [1] 24 25 26 27 28 31 32 35 35 As we have 9 values, the median will be fifth value, in this case 28 degrees. Let’s verify it by using R function which calculates the median automatically. median(summer_temperatures_celsius) ## [1] 28 Our visual calcululation was correct. If we think about it intuitively, does the calculated result make sense as an average summer temperature? Indeed it does, the median in this case did better job than mode. As you see, it depends on our population which of the two does better job. Will median help us with our third question of providing average salary though? Here is our data regarding salaries in the country. They are already sorted from smallest to largest for simplicity. salaries_country &lt;- c(350, 400, 450, 470, 500, 520, 1200, 1270, 1300, 1460, 1500) If we now calculate the median: median(salaries_country) ## [1] 520 The answer does not seem perfectly representative from our intuition. Yes, indeed the middle value calculated by median is 520, but it does not quite grasp the salaries which are above 1000. Luckily, we have our final tool among measures of central tendency - mean. Mean is a bit more tricky measure to calculate. By calculating mean, we find a point in the exact middle of our points regardless of how our salaries look like. The calculation is straighforwad: To calculate mean, sum all the points and then divide the result by their count. That is not so bad, we could do it with calculator or even on the back of an envelope. Firstly, let’s get the sum of all observations: 350 + 400 + 450 + 470 + 500 + 520 + 1200 + 1270 + 1300 + 1460 + 1500 ## [1] 9420 We can count the points (salaries) just visually and know that there are 11 salaries provided. We can then do a final calculation: 9420 / 11 ## [1] 856.3636 And finally we can verify this result by calling an R function which calculates the mean. mean(salaries_country) ## [1] 856.3636 The mean of approximately 856 seems a more reasonable result compared to a median of 520, to represent an average salary in this case. "],
["measures-of-central-tendency.html", "Chapter 3 Measures of Central Tendency 3.1 Level 1: Being Aware - Reached", " Chapter 3 Measures of Central Tendency As you see, neither of the three measures of central tendency is a silver bullet. Their success depends largely on a population which we are trying to describe. That is why it might be a good idea to look at all three of these and then face the provided answers with our intuition. The three measures - mode, median and mean; are actually not the most important piece of learning now. The fact, that we know our did our very first application of Data Science is! We started the chapter by saying that the aim of Data Science is to create valuable information out of the data, whereas this information has value for someone (our employer). Maybe we are working for a bread producer and our tasks is to find our how can the produces differentiate in the market - now we know that it should not be 500 grams bread. Maybe our employer is a sunscreen manufacturer and wants to figure out for which temperatures the sunscreen should be optimized - now we know that fo 28 degrees. Or maybe, our employer is the government trying to figure out which income groups should pay lower taxes - could be somewhere below of 856 Euro. 3.1 Level 1: Being Aware - Reached Congratulations! Without even realizing it, you have reached the first level which I call “Being Aware”. You are now aware of what Data Science is, what is isn’t, what it struggles with and what is the role of Citizen Data Scientist. Here are some key takeaways which you should have from this chapter: Remember the five cornerstones upon which this book is built. If you plan on continue reading it, these will be helpful to keep in mind. Data Science is an art of creating valuable information out of data. No training or online course can really prepare you for real world. Hopefully this book will help you to go to field as soon as possible and apply yourself, based on a level which you reached. Data Science attempts to often times describe a population. This can be done by measures of central tendency, such as mode, median or mean. "],
["observer.html", "Chapter 4 Observer 4.1 About this level 4.2 Be Skeptical About Data 4.3 Data Science Project 4.4 Your Contribution 4.5 Data and Computing 4.6 Using Data 4.7 Descriptive Statistics 4.8 Predicting The Future? 4.9 Basics of Machine Learning 4.10 Supervised Learning 4.11 Pitfalls of Machine Learning 4.12 Regression 4.13 Classification 4.14 Level 2 Reached", " Chapter 4 Observer 4.1 About this level Welcome to Level 2: Observer. Once you reach this level, you will be able to act as an Observer of a Data Science project. You are not yet directly able to contribute to it (such as through coding), but you will be able to use outputs of it and help with the inputs. Thus if you are a campaigns manager in marketing department, you will know what to ask Data Scientists for and how can you use it. Also, you will have some basic tools with which you can evaluate whether the Data Scientist that your initiative is dependent on did a good job or not. As I mentioned, you will also be able to help out with inputs. Whether you know it or not - Data Science is dependent on people like you, who have the context knowledge. That is why there is the famous saying, “Garbage in, Gargage out”. If you will be able to point a Data Scientist to what he should use and what he should not use, he will be grateful (unless he is stubborn). We will need to develop your skills in three different areas: Soft understanding of Data Science Be Skeptical About Data Data Science Project Your Contribution Programming Data and Compuing Using Data Statistics Descriptive Statistics Machine Learning Supervised Learning 4.2 Be Skeptical About Data Are data really powerful? Being skeptical about the power and bias in your data is the most crucial skill, or mindset which you will aquire on this level. Even experienced Data Scientists sometimes forget this point. Firstly, let’s think about the power of your data. It might be necessary to ask, whether the data even can answer the questions we intend to ask. Let’s say that you got a task of optimizing bus transportation in your city, fairly large city let’s say. The optimization should bring a benefit that the number of busses on each route should be adjusted to the demand, making passangers happier, thereafter using the busses more, earning you money and helping the environment. The dataset which you have available are: Every bus route and number of busses operating it. Every bus stop and a delay that every bus had on that particular stop. Every ticket bougth - at a bus, at a stop or online. Do you think that this dataset will be sufficient to fulfill the task? Figure 4.1: Is our data powerful enough to optimize the bus routes? You do a perfect Data Science project, utilizing the dataset to its maximum. Some of the things you will do based on a dataset: Bus routes, where people buy most tickets on their stops will increase in number of buses operating them. You are hoping that this way, the busses which might be crowded will be more comfortable for the large number of people using this route. Bus routes, where buses tend to be late are rerouted. You found some alternative paths, alternative places for a bus stop and are hoping that this will decrease the delay which buses have, making people more happy. Seems reasonable right? There is though a catch and let’s see what might actually happen as a result of your Data Science project. What will most likely happen is that you might optimize for the opposite. You have increased the buses for people who only occassionally travel with a bus, such as pensioners when they go for a walk in city centre. You have not optimized for heavy users of bus transportation - commuters who daily commute to and from work. Similarly, your change in routes meets opposite of success. The reason why the buses are late on these routes is that these are the major paths for people in the city to use. Why have we failed when we did a perfect Data Science and our assumptions about the bus routes seemed to be very reasonable? The answer is that our data did not have the power to correctly solve the problem. What we mainly lacked was the online purchases of tickets as well as yearly subscriptions to public transportation services. Both of these ticket purchases are untracable. As a matter of fact, people who buy yearly subscriptions are the heaviest users of public transportation and we really have no data about their movement. Thus, our dataset simply did not have the required data. One way to solve this would be to change the system in itself. For instance, even if you have a yearly subscription or a ticket bought online, you have to tag it once you enter a bus. It won’t wihtdraw any money from your account, though your yearly ticket will be invalid, unless tagged. The message which I wanted to translate with this short example was not how to optimize public transportation, but so that.. you shall be skeptical about the power of your data. To demonstrate this on my experience, I was once working with digital banking application and we intended to create budgets to many customers. These budgets would be helping you save money and have overview of how much you are spending on various categories such as groceries, going out or vacations. Even though I was trying various methods it was not working out well. The budgets were absolutely unstable and customers every month spent different amount on things. How is it possible? Of course my data did not have the power to answer the question I was given. Even though we every month consume similar amounts of things, we do not necessarily buy the same amount due to simple reason - we share with someone. Usually customers have families, wifes, husbands, friends, flatmates and they share expenses among each other. To solve the task properly I would need to link together the people who share their expenses and only then I would be able to answer the question. Are some data specifically biased? When it comes to data, there are two areas which I tend to distinguish: Machine-related Data that relate solely to machinery or natural processes, often times collected through sensors. You are a chemical engineer running trials and observing time for a certain reaction to happen. You could be a mechanical engineer watching whether an error won’t occur in your machinery. Assuming that your sensor is reliable and well setup, your data should be unbiased as both machine and nature (in this sense) are stable in behavior. Human-related Data that display human behavior. Self-reported data, such as surveys. Imagine anything from voting polls, through asking whether someone wants to buy our product, up to even reporting an adress of residence. Collected data, such as logs from mobile application which are tracking human activity. To answer our question whether some data are specifically biased, simple answer is YES. There is a lot of data from my perspective which are just wrong to be used and you as a Citizen Data Scientist should not use. I do now wish to talk now about data quality issues, but about the nature of data, which is connected with how the data came into existence. There are a lot of datasets which came to existence by asking people regarding their state or opinion. Take elections for instance. If you look at US elections in 2016, for a very long time it was seemingly impossible that Trump will win the elections. Yet, he won, even though the data from polls told us otherwise. What happened is that this (poll) dataset was created by asking people, while unfortunately it is in human nature that: people lie with purpose people are lazy people cheat people want to appear different than they are people do not know themselves I might seem to negativistic again, but this is what my past has taught me. Points which I am mentioning above are in our human nature and now it depends upon whether context will push us into one of these biases. I will bring several examples, but for now let’s stick with these. Due to these biases that affect self-reporting, when your Data Science project is working with people, you should not rely on data which were created through asking and self-reporting. I will be honest, from my perspective all survey data are just wrong and in nowadays world, I would simply skip this practice as we have better means of data collection. Just last week I headed to my bank as a client with the intention of closing my account. I arrived at the branch (which was completely empty due to early hours) and took a place at the advisor. Upon my announcement, the branch advisor said - “You have a different bank?”, to which I replied positively. When my churning papers were printed and ready for signature, I noticed that there is an explicit field stating a reason for churning, where of course was “Client has different bank”. There was actually a list of reasons why I was churning - unprofessional staff, bad mortgage offer, bad credit card offerings and expesive services. As a client, I did not argue and let that reason be there. As a Data Scientist though, flames of justice were burning inside of me! Imagine the poor Data Scientist who receives a task to analyze data and determine what causes churn. Oh dear… Do you think that the bank is not aware of an issue with human biases, such as this one? Of course it is; and so cross-checks are present, when a second advisor needs to arrive to confirm the actions of their colleague. Do you think that the second advisor cares more than the first one about the quality of their data? Of course not. Which of the points above occured in this situation? I was lazy and I did not know myself. I could tell the bank clerk that having another bank is not the real reason, but I just did’t feel like it. Also, I don’t really know why am I leaving this bank. One morning I woke up and I felt like I do not want to be their client anymore. My branch advisor was I think cheating. He was happy that I agreed to the reason that I have different bank. I could have used one of the reasons that I am unhappy with the service I am receiving at this branch. This would of course cause his colleagues troubles. Figure 4.2: Remember that people have a lot of internal biases and will appear diferently than what they really are. Photo by Juan Pablo Serrano Arenas from Pexels Thus if you end up as the poor Data Scientist in this case, you should seek ways how to overcome the biased self-reported data. Let’s assume you work in a bank and your task is to develop a Data Science model which will automatically be granting loans. As you are working within loans department, you very well know that the primary factor for whether your bank will grant a loan is the income of a customer. You also know that there is a field in your database that says “income” to each one of your customers. Problem solved right? You just collect a few more fields and create a model as you were asked. Well, I recommend something different. If you will ask around you will find out that this “income” field was created through self-reporting of your customers. When they are setting up their account, the advisor in a branch will ask them what is their income and they report figure X. Trust me when I say, that due to one of the reasons listed above, the customer will report a wrong number. Ok, then - you tell yourself that you are smarter than that and decide to observe customer’s incoming salary transactions which you have tagged in your system. You have overcome the problem of self-reported information, right? You deploy your model into production and run your automatic loaning system. What will happen is that in some time frauds will arise and there will be a lot of customers who took huge loans and do not pay them back. How could this happen? Well, people learned how to cheat your model for their own benefit. Fraudsters are now once a month sending each other transactions which are tagged by your system as salaries and thereafter your “income” field for this customer is huge. They are then automatically granted significant loans. So, how can we solve this? The key is to obtain information which is desired through data which were created by human, while he was doing something completely natural and at best, unrelated. You need to think about an action, or a process of a customer though which he/she unknowingly displays his/her truthful income. What about groceries? We all do grocery shopping, thereafter this might be a reliable source of information. What if you look at how much this customer is spending for groceries monthly and relate it to his/hers assumed income. If you have reported 1500 Euro income, how comes that you only spend 100 Euro for groceries? I would rather presume that your income is somewhere around 500 Euro. This is of course not a perfect solution, but what I am trying to translate is the skeptical thinking about how to solve the problem with bias about human-related data. 4.3 Data Science Project How does Data Science project create benefit? Let’s start from the very end, what should a project generate. It should be new information, in various form, that creates benefit for a person or company that financed this project. Let me emphasize the point of that the project creates new information, not new data. This is because data by themselves, have no benefit, and thereafter there would be no point in running a project that creates data and not information. This information can be in various forms, so let’s take a look at some examples: When I was at university I was tasked with creation of project, which would take recordings of people walking and it should create an information of whether people on the recordings are suffering from Parkinson’s disease or not. Having this information created various benefits, for instance (if working properly), expensive time of doctor is saved. Ultimately, people could self-administer this test at home without the need of visiting the doctor. The information created was the statement “Has Parkinson disease” or “Does not have Parkinson disease” to every recording. Once I ran a project which was supposed to determine a location of newly opened retail stores in a country. The input for my project was the network of roads and cities in the country with their respective counts of inhabitants. The information which my project was creating was a potential position of retail stores that would be optimized for a distribution of roads and where people live. This information created a clear benefit - competitive advantage for particular retailer. Numerous times in the past I was asked to determine customer’s probability to purchase an item. My project always took as an input various customer data such as their demographics while it predicted a probability to purchase particular item (information). This information has a benefit that a business representative can then decide whether he/she will invest into marketing for particular customer. Figure 4.3: My showcasing how Data Science can diagnose Parkinson disease, through recording of gait on four points on the body. I could go on, but I guess you already have the picture of what is the single requirement of any Data Science project - creating an information from which we benefit. The beneficial information can now really vary in forms. It can be anything as you already saw, but I would like to broaden your horizon even more. The output of a Data cience project can be a single diagram, or even a sentence (thereafter it does not have to be predictive modelling or optimization only). How does Data Science project look like? Essentially, all Data Science is about are hypothesis. This is how hypothesis is defined by Oxford dictionary: a supposition or proposed explanation made on the basis of limited evidence as a starting point for further investigation. You would be surprised by how many Data Scientists are forgetting some parts of this definition so I would like to focus on it. At the beginning of a project you suppose something can be done - “We are able to find ideal locations for our retail stores.”. Then you examine whether you already do not have enough evidence which would answer your supposition - “There is no other retailer similar to us in a given country and as we are new to the country we also have no knowledge.” Finally, as we already discussed, there is a clear benefit if our assumption is fulfilled - “Once ideal locations are found, our stores will be easily accessible by customers and this will be profitable”. Once you work out your hypothesis, your Data Science project can outset. You will seek for available data that you can use to assess your supposition. In later chapters, you will learn about concepts such as Minimum-Viable-Product or experimentation that will help you to set up as simple way as possible to assess your supposition. But, let’s leave it hear from now - remember to always set up a project based on your hypothesis. 4.4 Your Contribution How can I contribute to Data Science project? I have to make a small assumption here about who you are. You are someone who works in certain domain - for example banking, paper industry, insurance, car sales, anything. You either have formal education in this domain, or working experience. This means that you have something that no Data Scientist has - specific knowledge of the domain. Thereafter, as we already mentioned, you can contribute to the Data Science project in two ways already on this level: As Observer, contributes through inputs and through outputs of Data Science project, not into the central process. As you already know, the basis of every Data Science project are data (input). As you have domain knowledge, you might now about data that Data Scientist is not aware of. I will demonstrate this from one project of mine from the past. My project was using log data from a mobile application, whereas users had to go through several screens in order to complete purchase process. I collected all the log data and what it was showing was that a certain group of users was dropping of from the sales process at various point. I was struggling with it and I was unable to explain why this group of users (which had no clear characteristics) was dropping off at various times during the day. I was then approached by a call center representative, which for several years sells the same product over the phone. He did not even have to look at the data and gave me the answer! These were parents of young children! As the sales process takes an hour to get through all the screens, there is a solid probability that if you are a parent of a baby, the baby will start crying and you will of course leave your computer. The data would never reveal me this pattern, the domain knowledge did instantly. You can be the gentleman from call centre, which moves the project a light year ahead! Figure 4.4: Crying babies were a patterns that data would never reveal to me. Photo by Ba Phi from Pexels. Secondly, every data science project should produce an information (output) that will be used to create benefit. This component of the Data Science project is often times underappreciated and you can be the one who fixes it. I will now go back to my projects where I was supposed to predict the probability that customer purchases a product. I was given this task by our CRM department to optimize email campaigns - so that these are sent only to customers who have decent probability to buy the item. Everything worked well, the Data Science project ran through, the information was created. BUT, the created benefit was close to nothing. As it later turned out, customers are generally overfed with emails and digital channels. I then met with a representative from our branch segment and he said that this information could be of great use for our sellers. The sellers have only very limited time when the customer approaches them and thereafter having the information they will be able to sell more. The data, nor my knowledge of Data Scientist would never reveal this use of the information which I created, the domain knowledge did. You can be this representative, who will multiply the created benefit our of Data Science project. 4.5 Data and Computing How is data represented? If you plan on skipping this chapter, because it seems boring, please don’t. It would come to haunt you in your glorious future of Citizen Data Scientist. Data Science is done using computers, thereafter I think it is crucial to understand how your computer sees data. We often times think that computers are smart, while in fact they are quite basic - they understand only two values - 1’s and 0’s. In fact some of the earliest computers were presented data through punchcards. Figure 4.5: Example of how one of the earliest versions of computer from IBM saw data - as punchards with holes (1s) and not-holes (0s). Picture by dansimmons on Twitter How is it possible that two values are enough to represent anything to the computer? Through converting of numbers into binary form. I do not want to bother you with the details of this method, I just want to tell you that any number is easily represented in a binary form. Figure 4.6: Example of how texts and numbers can be encoded into binary. Now what we often times make mistake with is that we overestimate powers of computers and we think that they understand anything - texts, colours, pictures. Well, they don’t. They only really well understand numbers as these can be easily converted into binary form. Thereafter a lot of tasks of Data Scientists revolves around representing data well for the computer. If a Data Sciene project intends to work with images or texts, one of the most crucial tasks is, how do we convert these to numbers, so that they can be represented well for computer in binary forms? I was on a conference where a Scandinavian bank was representing their project of revealing fraud transaction by the means of Data Science. Transaction usually looks as something like following: Figure 4.7: Example of how we view a transaction. However what a presenter showed was something like following: Figure 4.8: Example of how it might be necessary to alter transaction so that our computer views it correctly. One of the keys of their project is how the transactions were represented to the computer (and algorithm). We will get more into this later. Important message that I would like you to learn now is that; data representation is one of the key components of successful Data Science. Usually it will be rather simple to represent data to an algorithm. In the next chapter we are going to learn about data types and in most of the cases this will be sufficient. Though as you will be meeting some more complex projects, such as Natural Language Processing, the topic of data represetation will become more prevalent. How does my PC handle data? As soon as you start a Data Science project, you will be facing hardware limitations. Simple reason for this is that hardware costs. Sure, you can purchase a hardware and have no variable costs, but this hardware will be getting outdated or you might incur opportunity costs. Thereafter, every Data Science project will be optimizing for as little hardware requirements as needed to fulfill particular task. These requirements will be coming from four crucial componenets that a computer has and I think it is hence crucial for you to learn about them now. In order to process data, and run Data Science project, following four components will be needed and this is how I think about them: CPU (central processing unit). This is a place where all the calculations take place. When you will work with algorithms which are computation heavy and are dependent on CPU, you will run into limitations. RAM (random access memory). This is memory which is instantly accessible to all the programs, thereafter it is super useful to our work when we need to work on some data. When you will work with larger datasets, RAM requirements can become problematic. Harddisk. This is your “bookshelf” of data. Once you are done working with your data, you store them here until next time you need them. Usually, harddisk space does not pose limitations to your project, as it is fairly cheap. GPU (graphical processing unit, or graphics card). This is an alternative to your CPU, which is considerably more powerful and will come in handy in late chapters of this book. Figure 4.9: Remember that for a Data Scientist hardware matters - mainly CPU and RAM powers. What is a csv? Now that we know what data is, we also know what we aim to do with it and how will we manage with our computer, it is time to get our hands on some data. The simplest representation of a dataset is through so called “csv” or “comma-separated-values”. In plain form, these might look something like this. Figure 4.10: One customer representation in csv form. It is quite ugly right? The reason is that data in this form are not meant for a human eye, but for programs to read which can in turn display the data in nicer way for us. Later on when you will be importing such data, you only need to specify two things so that our program knows how to read it in well: Separator (what separates columns), in our case ,. Whether first line is a header (names of columns), in our case yes. If you ever worked in Excel, when you are saving document, you can decide to save it as “.csv”. This is by the way a good practice as the environments you will later on learn to work with can read it easily. What is an observation and a feature? Data Science project will be aimed at certain phenomena which is of our interest. This can be our customer base, weather, how certain machine operates, how our colleagues work - anything. In order to apply Data Science to this phenomena, we will need to describe it through observations and features. Observation is some small unit from our phemena such as a day, one customer, one colleague, one moment of machine’s operation. Secondly, feature is characteristic of an observation, such as temperature in a given day, age of one customer, arrival of our colleague to work or speed of machine’s operation. Here is a small table to summarize it: Phenomena Observation Feature weather day/hour/minute humidity customer base one customer age of customer Throughout your entire career, you will be meeting these two concepts of observation and feature. These will vary in forms, but the way we described it now will always hold. Thereafter we can adjust our csv file and we can represent in it one customer with some of his features. As a standard, features are displayed in columns. Figure 4.11: One customer representation in csv form. What is a “flat table”? We now leared how to store data about one customer, but we have many customers right? To store data about all of them in reasonable form, we will use a flat table. It is common to store observations in rows. Thereafter, we can just add several rows, representing several customers. Figure 4.12: Multiple customers representation in csv form - flat table. What is granularity? Even though we can now store customer’s features nicely, we still haven’t won due to the issue of granularity. Some of the features about a customer will have different granularity level and we are unable to store them in the same table in a nice way. What if we wanted to store cups of coffee per day, per customer. With our current knowledge we could do something like this. Figure 4.13: It would not make too much sense trying to represent data on different granularities in this way. Ugly right? It would be hard to manipulate such data. We need to solve this by introducing more tables, whereas these are on different granularity level. We will call this relational database. What is a relational database? Edgar Frank Codd at IBM in 1970 in his paper “A Relational Model of Data for Large Shared Data Banks”, introduced a concept of how the problem above can be solved in a neat way. Luckily for us Data Scientists, Oracle implemented this idea into a working software solution in 1979 (Ever wondered how that company became so successful? This is it.). We create two tables, with different granularities separately. The only extra point which we need to specify is how are these linked together - for example customer name. Figure 4.14: Correct representation of data on different granularities - relational database. When you will be contributing to a Data Science project, keep the data granularity in mind. If the data of different granularities enter the project, it is neccessary to harmonize their granularities. This can be done for instance through data aggregation which you will learn on next level. 4.6 Using Data Can we already do Data Science? Now that we constructed ourselves a small relational database with everything that we know about our customer base, are we ready to do Data Science? Yes, so let’s give it a try. We are a coffee seller and we are interested which age group drinks most of the coffee, so that we can better target our marketing efforts. calculate the example by hand Do I have to do it in this cumbersome way? Luckily, software can of course help us. Now it is time to turn into learning what software can help is and how. We will need an Integrated-Development-Environment (IDE), engine and maybe a library. What is an IDE, engine and package/library? Even though you are right now at the level of Observer, you might already come in touch with programs or libraries which make Data Science projects. Thereafter, it is important to understand a terminology here. The most obvious are so called IDEs - Integrated Development Environments. These are pretty and easy to work in. Currently, some of the most popular ones among Data Scientists are R-Studio and Jupyter Notebooks. This will be a program which runs on a computer (either a physical one or somewhere on the server), that allows you to utilize some computing engine. There are two very popular engines, which are actually competing with each other - R and Python. Imagine these as a set of functions which are very robust and people like to use them because they are well developed and once a Data Scientist works with them, he can be sure that his work will run. Finally there are libraries. The engines as weel as IDEs are many times licensed under open-source. This makes a lot of sense, because people have a motivation to contribute to these. How will they contribute though? It would be very tricky to integrate work of thousands of people though single software - may it be R or Python. For that, we have libraries. These are way smaller sets of functions, which usually serve a specific purpose. For example, even though R serves Data Scientists, you might decide to write specific library which will be for Data Scientists which are trying to predict zombie apocalypse. You will include a set of specific functions in your library and you may decide to publish it so that other people can use it. What is R? R is engine (or programming language) in which a lot of Data Scientists today work, because of its simplicity and statistical powers. We are able to solve our task with it with several lines of code while we make use of functions which someone else predefined for us. Here is how a solution looks. insert R solution to the problem Let’s talk a little bit about what happens. The function mean comes from a predefined library called base. It’s a bit ugly though, as we did not use any pretty IDE. We can write the same code in R-Studio IDE. This is how it looks. insert R-Studio solution Data Scientists of course like this as you can visually distinguish between various elements. The IDE will have much greater functionalities compared to only working in programming language, but we will get to that later. What is Python? you can download it with. What is Knime? Usually when people talk about Data Scientists, they mention two languages (or frameworks) - R and Python. I however believe that in the future a different approach will dominate if we would like to grow the numbers and powers of Citizen Data Scientists. These are frameworks which does not necessarily require coding, but can do very similar things (at a tradeoff of being slightly less flexible). One such framework is Knime. When thinking about Knime, we can forget our distinction between engine and IDE, here we get both things in one. One important difference to R and Python is that here we are not expected to explicitly write code in order to fulfill our data science task. Let’s give it a try. insert Knime solution As you can see we are again doing the same steps, just this time they are represented with visual modules instead of lines of code. Should I use R, Python or Knime? As you are on the level of Observer, you should not choose any. At this point, you should only be able to observe a Data Science project, thereafter when your colleagues say that they work in certain language (most likely one of these three), you know what they talk about and you can see ona very basic level what they are doing. 4.7 Descriptive Statistics In the last chapter we discussed that one of the simplest ways for creating valuable information with Data Science is by describing some population. It is now time to revisit this idea and expand on it. 4.7.1 Distributions Learn about a picture of distribution. 4.7.2 Percentiles Where in population you lie, to compare yourself. Quantiles Boxplot is another way, how to show distribution of population. 4.7.3 Mean vs. Median End with that it is outliers and extreme values which are influencing. Let’s learn what to do with them. 4.7.4 Extreme Values and Outliers How do we find them in boxplot. 4.7.5 Measures of Spread Range IQR 4.7.6 Percentages the danger of working with them (CRM campaigns doubling conversion rate) they are relative to some amount (which can be small) 4.8 Predicting The Future? Predicting without Machine Learning - importance of non-ML baseline 4.9 Basics of Machine Learning What is Machine Learning? Machine Learning is crucial part of Data Science. Often times when I see courses or training materials, this area is postponed until the student learns a lot of basics, such as data preprocessing. Only then, it is believed, you should be exposed to Machine Learning. I believe the opposite - you should meet it already now (as an Observer), due to two reasons. Firstly, as already mentioned, Machine Learning is something like a hearth of Data Science. Secondly, it will keep your motivation higher - you need to know some cool things that you will be able to do once you finish this book. To explain you properly what Machine Learning is, I need to go into history of computing. Why did computers arise? They arose to simplify or automate tasks done by humans. One such task was to summarize consensus data (the survey done every 10 years which observes a population of a certain country). The problem in 1880s was that it took over 8 years to summarize and process the data from a consensus, thereafter making it quite worthless due to the length of the process. One of the clerks, Herman Hollerith proposed a solution through a counting machine that successfully sped up the process. Figure 4.15: Data processing as a use of computers. Picture by IBM History. See at: https://www.ibm.com/ibm/history/ibm100/us/en/icons/tabulator/ This was a primary solution for over half a century until commercial electronic computers arrived. Having computers with certain rules which were doing what otherwise humans could do, was really helping humanity. This automation was done by telling computers explicitly what to do through rules. Rules have problems though - they have hard and strict boundaries. If you want to design a great rule based system, you need to be really careful so that you encode all the patterns which are in the data. With the arrival of commercial electronic computers the rule based systems were becoming more and more complex. Firstly, it is very hard to encode all the patters into our model. Secondly, our population and data change over time. Due to that, even if you manage to encode all of the rules, very quickly will these become outdated simply due to changing nature of our world. The story of Machine Learning continues at IBM, as here in 1952 Arthur Samuel came up with the first phrase of Machine Learning. He was interested in designing a computer program to play checkers, while the electrical computer had only a very limited memory available. He was thereafter motivated to write functions which will analyse the game on the go and decide on the next move. You see, this machine no longer had explicit instructions on how to work. If you open the Wikipedia page about Machine Learning, most likely it will be defined through the definition of Arthur Samuel from 1959: Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Figure 4.16: Arthur Samuel is one of the key persons in the history and evolution of Machine Learning. Picture by IBM History. See at: https://www.ibm.com/ibm/history/ibm100/us/en/icons/ibm700series/impacts/ Here we see the difference - instead of explicit instructions, we are going to seek for patterns and inference in our data. By doing so, we will not have to maintain the model manually anymore, like we had to with rules creation. Also, these patterns will have soft boundaries and conditions will not need to be met exactly, but a case needs to come close. Are we already at the definition of Machine Learning how we know it today? Not quite. You see that even though we are able to let the program decide through identified patterns, instead of explicit instructions, we are not generalized yet. The computer of Arthur Samuel was designed to be good at specific task and we cannot take it and generalize it to playing cards for example, or optimize bus transportation. The next leap forward came from Frank Rosenblatt in 1957 who developed the idea of perceptron. Even though it took many years after Rosenblatt introduced the concept, the idea which he proposed was crucial. The Perceptron is a function which could be used for many tasks and not only for one specific. Figure 4.17: The Perceptron was the first version of Machine Learning model as we know it today. It could theoretically be applied to any task at hand. Let’s say that you are running a canteen where a few hundred people arrive every day for a lunch. You are supposed to optimize which meals are being served, and also how many portions will be cooked. This should yield a benefit of lowering waste created in your canteen. How will you go about it? Your Option 1 is creation of certain rules. You look into your past and notice that your canteen never sold more than 400 meals in a day. On the other hand, you never sold less than 100 meals. This will be your first rule, to always cook between 100 and 400 meals. Whenever you serve a national dish - Wiener Schnitzel, you are always over 300 servings, people love it. However, if you serve Wiener Schnitzel 2 times in a week, the second time only very few people come, even though people love the meal, they want it only once a weak. There is your second rule. If you serve rice as a side dish, some 30 people less always come as compared to potatoes, people tend to like rice less. There is your third rule. Now you can be working through a rule creation for a very long time and construct a list of explicit rules according to which you will operate the canteen. Your Option 2 is use of Machine Learning. You will collect as much data about the past consumption in your canteen. Often times people make a mistake of thinking that I only throw data into Machine Learning algorithm (such as The Perceptron) and it will save you a lot of time. No, you still need to invest your time, but into preparing the data for the algorithm, instead of creating the rules yourself. Thus you will not only collect the meals, their consumption, but also things like public holidays, alergens in the meals. All of this will be nicely prepared, fitted into Machine Learning model, which will discover various patterns. Now whenever you will be planning to cook a meal, you will push it into your model, and it will predict what will be the consumption. Figure 4.18: Should we use rule creation or Machine Learning to optimize our servings at canteen? How do I use data for Machine Learning? The definition from Arthur Samuel from 1959 has a second part which might anwer just this question: Machine learning algorithms build a mathematical model based on sample data, known as “training data”, in order to make predictions or decisions without being explicitly programmed to perform the task. We are going to show our Machine Learning model a set of our data, which we will call training data. The underlying statistical and/or mathematical model will seek for the patterns. Once those are found, the model will be trained to recognise learned patterns. Once the model is trained, we can present some new data that the model has never seen and ask the model to make a prediction about this new data. Imagine a child which has never seen a monkey (untrained model). We will now walk the child through several zoos and show him various monkeys (training data). The child has learned to recognise and identify a monkey whenever it sees one (trained model). How is our trained model beneficial to us? We can show it unseen data, which we usually call unlabelled data and ask the model to make a prediction for these. This is the case when we would ask the child what it sees and it should identify a monkey. If we go back to our example with canteen: Training data is all our data from past which we can collect. We know how much of each meal was consumed. Unlabelled data for which we would like to obtain prediction is our plan of what meal to cook tomorrow. We want to know how much of our planned meal tomorrow will be consumed. Do I need Machine Learning? Now that we know what Machine Learning is, I would like to stop for a second and thing about whether we need it at all. These are questions that even experienced Data Scientists often times forget to ask and they end up with what I call “overkill”. If you are considering application of Machine Learning, the first thing you should ask yourself is whether simpler methods cannot answer your issue. The simple reason for this consideration is that Machine Learning could get expensive (we will get to the details in later chapters of the book). I will recall one Kaggle competition to give you and example (we will get to those later as well). The competition was regarding a mobile game for kids. This game includes various activities such as videos, games and assesments (tests). The task for a Data Scientist was to predict how many attempts will it take the kid to pass an assessment. Our training data were all the assesments played in the past by hundreds of kids. There was also unlabelled data, the most recent assesment of kids. There were many people who instantly jumped to Machine Learning and spent hours trying to apply it to the problem at hand, while thinking that their solution is pretty decent. All of a sudden a solution came which contained only 10 lines of code. All that the author of this submission did was that he looked at the average number of attempts that kids needed to solve a particular assessment. As there were only four unique assessments, he really only quickly calculated a mean out of training data. His calculation was hence only four numbers and he assigned to unlabelled data the one, which respective assessment a kid was about the solve. By making such simple predictions, the author beaten everyone’s Machine Learning solution until that day. He set a new baseline of what people need to beat in order to justify the use of Machine Learning. Keep this in mind and in the future always consider whether a simpler solution might not answer your task. IF this Kaggle competition was real world, maybe this solution would be sufficient enough, and by being extremely cheap, we would satisfy the requirement. 4.10 Supervised Learning What is supervised learning? Machine Learning has several subfields to it, on this level (Observer) you are only going to learn the most applicable one - Supervised Learning. Supervised learning deals with issues where we have so called target feature. Do you remember how we discussed observation and its characteristics? Target feature is going to be one characteritic, which is for certain reason of our interest. We are interested in training the Machine Learning model to be accurately predicting our target feature. The rest of the characteristics will be called independent features and will be used to predict target feature. The naming conventions are quite important to learn. The name independent features is coming from our assumption that these features are independent from each other and from the environment. That is why we can use them to predict target feature. This can be also called dependent feature as we are making an assumption that this feature is dependent on independent features. Figure 4.19: The first of Machine Learning that we will focus on is Supervised Learning. If we go back to our example regarding canteen meal consumption, this is an example of Supervised Learning. Let’s look at what we have as training data: Independent features: allergens in a meal day of week calories in a meal structure of the meal Target feature: number of meals consumed We can proceed to the training of Supervised Machine Learning model. As we said, we will pick one general algorithm available to us and feed it with our training data. This way, the model will become trained and we are hoping that it can make correct predictions for the future. As a canteen manager, I will plan the meal for next day in the afternoon of a previous day. I will decide that I want to cook a certain meal. This is what I will know about my meal tomorrow: Independent features: allergens in a meal day of week calories in a meal structure of the meal Notice, that I am missing target feature, as this is my unlabelled data. I will now reach to my trained Machine Learning model to make a prediction of target feature. The model will return for instance that it predicts that 255 people will come to consume the meal which I am planning to serve. Is target feature always straightforward? As you see, target feature is a crucial component in Supervised Learning. Is it always easy and straightforward to find a correct target feature? No. Often times, finding and clarifying a target feature is going to be among the hardest parts of Supervised Learning. If you remember how I was in previous chapter claiming that any lecturer or training who promises to prepare you with exercises for real world - target feature definition is actually one of the reasons. There will be numerous projects where you will really need to think hard to create a target feature, which will allow you to construct Supervised model. I was once working on a project within environmental studies. The project was focused on how human activity influences deer populations in certain Swedish forest. Even though the forest was large enough for a population of deers, the worry was that building small roads, electrical lines and similar man-made objects will influence the deers negatively. Figure 4.20: Can human activity negatively influence deers? Photo by steve-130217 from Pexels. Data were collected over the span of several years. Researchers did a specific method of pellet observation to determine in which areas of the forest the deer spend their time. During these years, few new roads were built and one electrical line. My task was to build a Supervised model which could be used in other Swedish forests, when there will be a plan to build roads or electrical lines. By having such model, environmentalists could stop constructions which would negatively influence the deer population. The question which now bothered me was what is the right target feature which I should use? One option would be to only look at whether the deer is or is not in this area of the forest. This is not quite right though. Deers move a lot around the forest and most likely, within one year they will visit at least once most of the areas in the forest. My model could then be too optimistic, saying that nothing influences deers and they happily run around the forest. Another option would be to predict how many times the deer pellet was found in particular area. Now I still needed to incorporate in my model, the change itself. The data were being collected over a span of time and I needed to see how my target feature changes over time. As you see the definition of target variable for Supervised Learning might be a really tedious process. With this chapter, I do not intend to give you guidelines on how to construct target variable correctly but to give you a mindset to… always consider and question whether your target feature is correct and appropriate We will get more into detailes and nuances regarding target feature on next level of Contributor. 4.11 Pitfalls of Machine Learning What is underfitting and overfitting? Machine Learning, and supervised learning are fields which have certain pitfalls. Now that we know how Machine Learning works, it is time that we learn about these. I will start the explanation with a quote from George Box, famous statistician: All models are wrong, but some are useful. As there is a lot of discussion in literature about this famous quote, I guess we can make our own interpretation. When it comes to the second part of this definition that some models are useful, we have already discussed this. You can build a Data Science model for essentially anything, the key is to build a model which will be useful and create benefit. What is more relevant for us now, is the first part of the quote - all models are wrong. However well we fit a model, it will never be perfect at predicting unseen data. We have a new term here - unseen data. This is simply a data that were not used during training of our model. That is why our trained Machine Learning model cannot be adjusted for this dataset. And, as it ususally is, unseen data is always at least slightly different to the original training data. Whichever process in the real world that you will be working with, the unseen data will posess some new nuances that your model cannot foresee. Let’s look at the following picture. Figure 4.21: Training data will always be different from unseen, unlabelled data for which our model should predict. Let’s relate to some examples that we already went through and observe whether unseen data will be different to training data: My model which was supposed to predict whether a person has Parkinson disease, based on characteristics of gait, has this issue. When a new person is observed, this person will have different height and weight. The walking style and clothing will also be different - no two persons walk or dress exactly the same. Our model of bus transportation optimization, will certainly have this issue. If we would fit a Machine Learning model, every day in a large city transportation system is different. Some buses will have mechanical issues and there will be also new traffic incidents on the road. You might be now slightly confused between the two terms - unseen data and unlabelled data, which we introduced earlier. Unlabelled data is part of unseen data. As this data does not have labels, we certainly could not use it for training of our model. I will clarify this further in the next section, though for upcoming lines I would like to ask you to clear your mind and think only about two terms - training data and unseen data. On the picture above, which compares training and unseen data, we can see the exact case. The data on the left, which will be used for training of the model are different from the unseen data. We are running into a danger of overfitting of the model if we don’t take this into account. Overfitted model is one, which is too adjusted to describe training data well, at the cost of loosing predictive power on unseen data. Let’s fit such a model so that we see what is happening. We fit a model which describes our training data perfectly. We then observe how good it is at describing of unseen data on the right. Figure 4.22: Overfitted model is great at describing our training data, but poor when it comes to unseen data. It is not great, right? The reason why is what I described earlier, your training and unseen data are always different. The solution to this is to aim for our model to generalize better. Model generalizes well, if it can retain predictive power not only on training data, but also on unseen data. We can try to fit such model, we will draw the line in a bit more ignorant way towards our training data. Figure 4.23: Balanced model describes our trianing data well and also generalizes well for unseen data. This looks more reasonable right? We call this a balanced fit as this model generalizes well. Being a little ignorant toward our training data allowed us to fit a model which has better predictive power on unseen data. We have to be careful to not be too ignorant toward the training data. If we were, what would happen is that we would fit underfitted model. Such model as bad at describing both - training and unseen data. Figure 4.24: Underfitted model is bad at describing both training as well as unseen data. The concept through which we just went is in literature often times called Bias-variance tradeoff. I do not want to teach you the idea through this terminology as I find it a little confusing and tricky to remember. Keep in ming the terms of overfitting, underfitting and balanced fit. What is validation set? We now now that we should avoid overfitting or underfitting of our model. How do we prevent it though? What we have in our toolkit now is only two sets of data - training data and unlabelled data. In the previous section we have used the term unseen data. We saw that in order to evaluate whether our model achieves balanced fit we need to have two sets, for which we know the label. That is why we need to introduce a new concept called validation data which will clear all of these terms in your mind. Let’s look at the following picture. Figure 4.25: An overview in terminology and use of datasets for training and validation of Machine Learning model. We start our process of thinking on the top - some data are labelled and some are unlabelled. In case of our canteen, labelled data are our past data of meal consumption. Unlabelled data is tomorrow - we only know independent features, such as what we are going to cook, we do not know how many people will consume it. Now we focus on labelled data, which we split into two parts - training data and validation data. A common practice is to use 20% of labelled data for validation data. In case of our canteen, we have data from the past 5 months labelled. We know how much of each meal has been consumed every day in the past 5 months. We cut away for instance the most recent month for validation data, so we will be left with the first four months as training data. Finally, we can focus on unseen data. We have learned in previous section that some data will be unseen to the model. This will be two sets - unlabelled and validation data. Unlabelled data are unseen to our model, simply because they do not have label and we cannot use them for training of our model. Validation data will be unseen to our model, so that we will be able to address the issue which we discussed in previous section - whether our model does not overfit or undeerfit. In case of our canteen, this would mean that we train the model on the first 4 months of the labelled data. We then use the most recent month to evaluate and judge whether our model does not underfit or overfit. If our conclusion is that it doesn’t, we can use it to make predictions for our unlabelled data (tomorrow) and be sure that the predictions made by model will be accurate. What would it mean if our canteen model is underfitted or overfitted? How would we know? In upcoming sections you are going to learn numerical and exact techniques of how to do this, but for now we need to stick to logic and intuition. We said that if model is overfitted, it is great at describing training data, but poor at describing unseen data. When we fitted the model, we see that it is making great predictions about the first four months of our meal consumption. In most of the days, it can really pinpoint the exact amount of meals which were consumed. For instance, when we made Spaghetti Carbonara, the model correctly predicted that 187 portions are going to be consumed. That sounds amazing! We use the model to make predictions for the most recent month, during which we one day cooked Spaghetti Bolognese, which are just different alternation of Spaghetti. We hence presume that our model will be again great. To our surprise, it predicts that 190 meals will be consumed, while in reality we sell only 110 meals. Our model is overfitted and failed to generalize to unseen (validation) data. We hence know that we cannot use it to predict our unlabelled data (tomorrow). We defined underfitted model as one which is poor at describing both training and unseen data. This one will be hence even easier to spot. When we cooked Spaghetti Carbonara, it predicted 130 meals, while really 187 meals were consumed. In the last month, which the model did not see, when we cooked Spaghetti Bolognese, the prediction by the model was 143 meals, while we really sold 110 meals. We can see that the model was poor at describing both of these meals. 4.12 Regression What is regression? It is now time that we expand our Machine Learning mind map. Supervised learning has several subfields, which are influenced by the form of our target feature. In this level, we are going to learn about regression and binary classification. Let’s start with regression. Figure 4.26: First area of Supervised Learning that we will focus on is Regression. In regression supervised learning our target variable is of numerical form. Here are some examples of numerical target features: Predicting income of a person. Values can be any numeric larger than 0 Euro, such as 580 Euro, 1561.6 Euro. Predicting cooling time of iron during forging process. Values can be any numeric larger than 0 minutes, such as 6 minutes or 257 minutes. Predicting weight change of adults over a period of one year of weight loss process. Values can be any numeric in range -30 Kilograms (Wau) to +30 Kilograms (Uhh). The way Machine Learning model which should be of supervissed regression type is going to be build is exactly as we discussed. We have a set of independent features, and our target feature. Let’s now build some really simple regression model. We are a real estate agent, selling houses in the suburbs of Stockholm. One of the really difficult things in our job is to evaluate what the property is worth and how much a potential buyer will be willing to pay for the property. We currently have only gut feeling and some rules which we developed over the past years of this job. Here is how we could evaluate price now: Largest factor that determines the price of a house is the inhabitable area. Price per square meter starts at 1000 Euro. Thus if the house has 100 square meters, we estimate that its price will be 100 000 Euro. If the house is nearby a lake or coast, the price increases. If the house is renovated in the past 10 years, or built within the past 10 years, its price will be higher. We can start building a regression model with single independent feature only (to make it simple), which will be the living area of a house. As we discussed we start with training data. To obtain these, we look at the historical sales of houses in the area. We make a simple plot, which shows how the area of a house and the houses final sale price worked in the past. This confirms our past experience, that the price of a house will be heavily determined by the living area. Our task is now to train the model. You will learn technical details of how to do this in the upcoming chapter (Contributor), we now only need to understand the logic. The trained regression model will attempt to fit a line, which will describe our training data well. Once the model is trained, we can forget our training data and are left only with trained model. In the picture below, this is represented by our regression line. We can now use this model to predict for us a price of a house, based on provided living area. The living are of our house of interest has a living area of 80 square metres. We look at our horizontal axis and find a a value of our independent feature - 80. We now draw a vertical line up and wait until it intercects with our regression line. That is the prediction of our model. The only thing we have to do now is that we draw a horizontal line to the left and read out the predicted price for this house. In this case, 67500 Euro. Figure 4.27: By inputting a value of our independent feature into regression model, we obtain its prediction. We can replicate this process on our dataset. We have now managed to fit a regression. In case you are headed to the more advanced levels, such as Contributor, you are going to learn exact techniques of how this is done, but as I said, on this level it is sufficient if you are aware of the conceptual steps when fitting a regression model. As we mentioned, the price of a house is of course determined by many more factors. In the upcoming chapters, we will also construct a detailed elaboration on this example of predicting house prices. How do I know if regression model is good? As an Observer, you need to be able to do a basic evaluation of a regression model, when you see one. You are not supposed to calculate it yourself, just observe it. The most basic evaluation of a regression model is through average error. How is this error calculated? Average error of a regression model is a bare sum of all the single errors the regression model makes, divided by number of observations. Single error is calculated as a difference between prediction and actual value. This definition contains a few new things, so let’s take it step by step. We have trained our model on training data and we want it to make prediction on unlabelled data. We have though already mentioned that part of our training data will be put aside and used as validation data. This is a dataset that we will use to calculate the average error of our regression model. Single error is what we obtain first. We take one house from our validation data and let the model make a prediction for this house. The prediction is going to say that this house will be sold for 87 000 Euro. As we know the actual sale price for which this house has been sold, we can compare the predicted value, with actual value for which this house was sold - 92 000 Euro. Figure 4.28: Single error (on one observation) is obtained by taking the difference between actual and predicted value. The very same procedure we now do for all houses which we are making prediction for. Figure 4.29: We repeat the same procedure and obtain a single error from all observations. All that we now need to do is to calculate average out of these errors. To calculate an average error, we only need to sum all of the errors made by the model and divide by number of observations. There is a small trick though. Some of the errors have negative and some have positive sign, as they were obtained using same calculation actual value - predicted value. If actual value is higher (80 000 Euro) than predicted value (67 500 Euro), the resulting single error will be positive (13 500 Euro). If actual value is lower (50 000 Euro) than predicted value (57 500 Euro), the resulting single error will be negative (-7 500 Euro). If we would now sum them all up, it would be a terrible mistake as negative errors would be cancelling out with positive errors such as: 13 500 + (-7500) = 6000 Euro. What Data Scientist usually does is that he/she uses squaring to the power of two and then using square root. This removes all the negative signs and we can happily sum up together all the errors to calculate average error. That is why when you will see a Machine Learning regression model reporting its accuracy often times say something like root-mean-squared-error (rmse). All that is happening here is that we are calculating mean-error, we just need to get rid of the negative signs. # Let&#39;s sum together all single errors which we did (gotten rid of negative signs). sum_of_errors = 13500 + 7500 + 5000 + 850 + 12000 + 3500 #Now we calculate mean (average) error by simply dividing by total number of houses which we are evaluating. mean_error = sum_of_errors / 6 print(mean_error) ## [1] 7058.333 As you can see, from numerical perspective, it is rather simple to evaluate how successful regression is. We can simply calculate the root-mean-squared-error which will already give us a solid estimate. In our case, this was 7058 Euro, which seems fine. Our model is far from perfect, but it can quickly give us an estimation of what the house is worth. Unfotnately we need to be a bit more careful, let’s again look at our predictions and actual values. Figure 4.30: We repeat the same procedure and obtain a single error from all observations. Sometimes, the overall view on the error, such as absolute error or average error can be rather misleading. If you noticed, three of the most expensive houses have been heavily underpriced by the model. This is caused simply by the fact that our regression model is not equally good at all levels of house prices. Now from context perspective, this can be a big issue as we are underpricing the most lucrative houses! Our lucrative clients can get pretty unhappy about that. 4.13 Classification What is binary classification? We can now move to binary classification, which is a supervised model, where target feature can have only two values. These two values can have various forms: True, False Buy, No-Buy Will run through, Will not run through Healthy, Not healthy Figure 4.31: Second are of Supervised learning that we will go through is Binary Classification. How do I evaluate if classification model is good? Discuss imbalaned classification issue. Discuss confusion matrix and predicting of what actually matters for us. 4.14 Level 2 Reached Congratulations! You have just reached level 2 - Observer. You are now able to join a Data Science initiative around you - it can be some public project, pro-bono project, or a project which is being developed inside your company. Moreover, if you are applying for a position related to a Data Science, you can claim that you are on Observer Level. This means that you know what is going on within the project, and you should be able to help with inputs, outputs, and do some basic evaluations of the project. Here are some key takeaways from this chapter: Always remain skeptical about whether your data can even answer the problem which you are trying to solve. Try to avoid using self-reported (human-related data) as these can be a subject to heavy biases. Remember that Data Science project tries to explain the phenomena through observations and their features (characteristics). Simplest form of representing data is through a flat table, in a csv format. If we have data of different granularity, we might want to use a concept of relational databse to represent these properly. Data can be loaded into into an environment which can help us with processing of it. These vary from ones which are programming heavy - R and Python, to some which are more visual, but can be less flexible, such as Knime. Machine Learning might be a hearth of Data Science. We train a model using training data, validate its predictive power on validation data and then use it for predicting unlabelled data. Be aware of not overfitting or underfitting of your model. Regression is a field of Supervised Learning, where our target feature is of numeric kind. We evaluate its success through measures such as root-mean-squared-error, though we are carefully looking at the single errors and their cotextual meaning. Binary Classification is another field of Supervised Learning, where our target feature only has two values. We evaluate its success through measures such as Recall, Precision or Accuracy. We though carefully look at confusion matrix. "],
["contributor-I.html", "Chapter 5 Contributor: Soft Skills 5.1 Soft skills 5.2 Project &amp; tasks 5.3 Fitting into a team 5.4 Legal regulations 5.5 Producing valuable output 5.6 Documenting your work", " Chapter 5 Contributor: Soft Skills 5.1 Soft skills You are now reaching a a Contributor level, at which you should already put your hands on keyboard and do some real Data Science! You are hence going to learn a lot about Data Manipulation, Statistics and Programming in this chapter. Before we get there, it is necessary to learn how are you going to do these tasks. In upcoming paragraphs we will answer the following questions: How should you as an individual approach given task or small project? How should you fit into Agile setting which your organisation likely has? How should you produce a valuable output from your efforts? How should you recognise importance of data regulations? How should you document your work once you are done? 5.2 Project &amp; tasks After completing this chapter, you will be able to (for example) import, prepare and preprocess data. What I often times see starting Data Scientists struggle with is that they get overwhelmed by the big picture of what they want to do. If you set yourself (or someone else sets for you) a project of “preparing dataset for modelling”, you will get lost, trust me. This job shall be splitted by you into smaller tasks for you to be managable. Figure 5.1: When you are given your first projects to work on, try to split it into smaller tasks. The picture shows what I often do. In this case, I was given a responsibility to prepare customer data and transactional data for modelling. This includes working with two tables - customer specific data and transactional data by these customers. I start on top and draft for myself two tables which I am supposed to process. Customer table contains one row per customer and transactional table contains one row per transactions, thus several rows per customer. Customer table will need removal of several unneccessary features (maybe I do not need to work with address of customers). Transactional table will need to be aggregated to a form of one row per customer. Afterwards, I know that these two tables will be merged, features will be plotted and extreme values removed. All of these things you will be capable to do after the Contributor level. Important learning is that you will be drafting yourself similar workflow pictures. It is quite likely that you will not yet be able to do such breakdown of the tasks or that you will be helpless with some tasks, which is perfectly fine - you should just ask for help. If there are more experienced Data Scientists around you, it is best to approach them. If not, there is always possibility to search the web for the answer to your issue. Remember one saying: Whatever the issue you are having is, there is always someone who had it before you and discussed it somewhere on the web. As a last resort, if there is noone around to help you, and if Google cannot find the answer, try posting your problem to forums such as Stackoverflow. People will either point you at past posts which were dealing with your issue, or help you solve it. When approaching your colleagues (or people on Stackoverflow), do not approach them with a problem, approach them with what is happening and what you would like to happen. If you are not showing or proposing a solution, you are just complaining, which is not productive. Some examples: Bad: I got this responsibility, I have no idea what to do. Good: I got this responsibility, I would need to split it to smaller tasks and draft my workflow. Bad: I have no idea how to remove features from a dataset. Good: I am working in Python, I have pandas dataframe, and would like to remove several features by using their name. Bad: I am not managing the deadline for preparation of this dataset. Good: I got stuck on plotting of features of this dataset, I am afraid I will not meet the deadline because of it. Now that you know how to split larger project into smaller tasks, and how to ask for help, it istime for actually sitting down and doing it. I want to give you advice also on how to properly sit down (and no I am not selling ergonomic seating cushions). Free up your calendar for 2-3 hours blocks, at least one or two of these blocks per day. These blocks represent a meaningful piece of work, and the way how you fill them is by taking a colour pen and looking at your drafted workflow. Each orange circle indicates one piece of work which should be done at once and takes you approximately one block. Figure 5.2: Having small tasks, divide your workflow into meaningful blocks of 2-3 hours. Judging by the fact that you are reading this book, most likely Data Science is not a bread-winning occupation for you right now, which means that your days are occupied by other things, such as meetings. Unfortuantely, Data Science cannot be done during breaks between meetings. From my experience, it takes 1 hour to get your mind properly to the problem, then 1 hour to do meaningful piece of work, and then 1 hour to properly wrap what you just created. I always try to have at least one such block within a day, when I want to do a lot of development on a project, I squeeze in one in the morning (8-11) and one in the afternoon (13-16). 5.3 Fitting into a team You have now learned how to approach a task as an individual, now it is time to learn a bit about what happens when several people are working on the same Data Science project - you need to oragnize yourself together. Even though you will be supposed to deal with the project from Data Science perspective by yourself, other people, such as business stakeholders will be collaborating with you. It is rather likely that once you join a Data Science project, you will be told that it is organized under Agile Methodology, or using Scrum. When I first time joined these methodologies, it felt like quite a mess. Due to that, I would like to dedicate a section to explain you what these are and how you will not find them as a mess. As you are learning on a Contributor level, and should already be directly contributing to Data Science workflow, these will become inevitable. Agile, Scrum or any other similar buzzword is simply a way of organizing you and your colelagues so that you are as efficient as possible. As you will grow your career, you will learn to realize that organizing people is a rather challenging task. There are multiple reasons for this among which; everyone has different motivations, different attitude, different personality and different skillset. This difference effect is even more multiplied in a field such as Data Science where for a successful project, you even need a mixture of roles. In basic project, you will at least need a Citizen Data Scientist, Data Scientist and Context Representative. In a more complex project, new roles will be needed such as Machine Learning Engineer, Cloud Specialist and Coach. That is why departments, divisions or even entire companies are turning to methodologies such as Agile or Scrum with hope, that these will solve the difference effect so prevalent among Data Science projects. 5.3.1 Kaizen The earliest in human history where I was able to track clear signs of what we today call Agile methodology is as early as 1960s. Clearest historical tracks led me to Japan and Toyota, with methodologies like Kaizen. Surprisingly, when USA wanted to land on the moon, they also used an approach posessing similar traits like Kaizen; and similar to what we consider modern today. I am purposely starting by a small class in history, so that you will have the core principles in your mind when you join all the buzzwording of today’s applications, such as Scrum. Let’s look at the definition of Kaizen by Oxford dictionary: a Japanese business philosophy of continuous improvement of working practices, personal efficiency, etc. Even though this definition is more or less decades old, it is what companies strive to achieve even these days. When you join a Data Science initiative, expect that continuous improvement will be expected, both from you as a person, as well as from the product which you are working on. One of the companies which have decades ago successfully adopted this methodology was Toyota, with their approach called The Toyota Way. They have developed a set of 14 principles. These range from principles such as respect for people up to use of pull systems to avoid overproduction. I would have to write an entire chapter to describe these properly. We will get to more details in later chapters, when as Project Responsible you will need to be aware of more practices in order to successfully lead a project. On a contributor level, this is not required though and I want to consider only on one of these principles - nemawashi. Make decisions slowly by consensus, thoroughly considering all options; implement decisions rapidly. 5.3.2 Nemawashi This principle is in my perspective a solid building block for involvement of Citizen Data Scientist (you). As it turns out, it is beneficial to have proper planning of what we plan to create. This planning should be done by multiple people (consensus), we should evaluate multiple options and then we should implement these decisions rapidly. Starts to sound like exactly what we need for Data Science project right? Let’s go through the elements one-by-one. Firstly, we will want decisions done by consensus. This of course does not mean that the entire organization should agree on what we are thinking to do - only the right people. In our case of Data Science project this could be: Citizen Data Scientist (You), who has overview of necessary Data Science priciples as well as context so that the the created project really creates benefit. Data Scientist, who consults and overviews some technical decisions made by Citizen Data Scientist whenever needed. Context Expert, who might be additionally working on the implementation of the project in the context. Additional Experts, such as specialists for Data Regulation or Data Logistics, as needed. As you can see, based on Kaizen, all of these people should be in an agreement of what will be developed as a Data Science project. Often times in nowadays companies, under Agile/Scrum, this will be referred to as squad or cross-functional team. It is simply the smallest team, which is able to make all decisions that are necessary and has all capabilities to develop and implement the project. Good team bad team examples If you will be asked to join a Data Science project, make yourself a mindmap of all the persons involved in a team close to you. If you conclude that your team can be classified as bad as shown above, try to communicate it and adress it. It is very likely that you will either not be able to implement the Data Science solution or that you will have unnecessary overhead (meetings, allignments, etc.). Secondly, we will want to thoroughly consider all options. We already discussed in previous chapters that this is really important for a Data Science project as this is an art of creating valuable information out of data. There will always be multiple ways how to solve this requirement. Some will be simple and their created benefit will be questionable. Others will be expensive, they will have a potential to create great benefit but their development will be risky as they are expensive. We will therefore, within our squad need a very proper planning. When the project is about to begin, you should be sitting together often and through various techniques be coming up with ideas, evaluating them and critisizing them. I was once starting a project which was considering optimization of company canteen. The canteen was every day preparing hundreds of meals for all the employees at the campus. Employee could pick from several options, which varied from classic national dishes, through fish, up until vegan dishes. What was always a challenge was to cook just enough dishes, so that the residual waste is as minimal as possible. If we did not remember our nemawashi principle, what we would do could be something like follows. We would request access to all the past meal consumptions and we would fit a regression model. The model would based on some features predict the numbers of each meals consumed. Most likely it would not be to accurate, but it would work somehow. If we did remember our nemawashi principle, we could find more options; and in fact we should. I did ask the chefs at the canteen about how they are dealing with the issue of waste. They say that for all the fresh meals they try to have accurate number, while there is one reserve meal, which is not cooked from perfectly fresh ingredients - such as spaghetti carbonara. All of the ingredients can be stored for at least a few days and most likely reused in other recipes. Even if there was a sudden spike in demand and all the fresh meals would run out, the spaghetti carbonara is of seemingly limitless supply. Good start on our Data Science journey! We know that we do not need to care about a demand of the reserve meal. As a next step I went to visit our canteen fr a few hours and I just observed, what is going on. I realized that almost everyone is an employee and there are almost no visitors. I had another option in front of me. Maybe I do not need to predict how many meals will be consumed, but only how many employees will come on a given day to campus. Note: The reason why I could not just collect data from entry cards about how many employees entered, is that canteen is purchasing ingredients the day before. This however gave me a completely new view on my options. Previously I would be limited to meal charateristics as my predictive features, now I could expand those into company calendar features. If a person has reported sick leave, vacation or home office, I can just go ahead and reduce one planned meal. The last set of options opened for me when I thought about meal preferences. I was at that time having vegan months, and though that some diets will remain quite stable - vegans, gluten free, lactose free, etc. I now knew that if a person displays a pattern of following some of these diets, I know exactly which meal they will take in the future! Once I did all of these considerations, I was able to lay all options on the table and make an informed decision of how to move forward. And no, you do not need to be fully developed Data Scientist to be able to outline all options for the project of Data Science, as often times, these come from domain knowledge. Finally, we will want to implement decisions rapidly. The question is, how rapidly? Simple answer is very. You may have noticed that many things in Data Science do not take too much time to try. We are learning concepts such as Baseline Model, which are techniques that are aimed at this principle. When you will have a clean dataset, it might take you as little as one day to test the assumption whether Machine Learning makes sense to be used. For larger decisions, such as cleaning of the dataset, these might take a week, which is still rather short time. This final piece of nemawashi principle shows nicely why we need a squad, which is able to do all decisions on the project and is able to implement them. Every dependency which you would have to other teams, or every regular management meeting that you would have to wait for a decision to be made, would terribly halt you from rapid implementation. 5.3.3 PDCA Cycle As you will see in a while, we will not only be planning at the beginning of the project, but also during the project, usually in two weeks cycles. Long time ago, many companies worked in a way of thorough planning for months ahead. Once the timeline and budgets have been approved, execution for months was ofset. This is not a case within Data Science, and it is entirely possible that plans will change in matter of two weeks. This was from practice of many companies concluded as shortest possible period for evaluation of our work. Even if some team members take a week long vacation, they will still contribute within the two weeks. I am hoping though for this to stick to your mind, planning of a Data Science project is absolutely crucial. Kaizen even developed a specific model to help us with the rapid implementation of ideas, called PDCA cycle. This is simply an abbreviation of: Plan Develop Check Act Such cycle might take as little as one or two weeks to be executed. Figure 5.3: Toyota and its implementation of Kaizen principles is a motivation for a way of working of Data Scientists nowadays. Resource from CNN, see at https://edition.cnn.com/travel/article/toyota-kaikan-tour/index.html As you can see, this single principle of Kaizen is already enough for you as a Contributor to realize how can you work efficiently. Let’s recap: You will work in a small team (usually 4-8 people). This team will be able to develop and implement entire project and will be making all the required decisions. Most likely, this will be usually called a squad or cross-functional team. You will do thorough planning and evaluation of all your options. This will happen not only at the outset of a project but also while you are developing it. This will be referred to as planning phase when the project starts and as sprint planning or grooming during the process of making the project. You will be rapidly implementing various ideas and testing assumptions. This is going to be happening on weekly or biweekly basis and will be usually referred to as sprint. 5.3.4 Agile &amp; Scrum Can we finally get to Agile and Scrum? Indeed! Now when you understand core principles that Citizen Data Scientist should stick to when he/she works, relating these to the words Agile and Scrum is rather simple. The principles which I describe above have been in the core of software development in one way or another in multiple systems - rapid application development, dynamic systems development method, scrum, feature driven development, etc. I am not saying that all of these are the same, but that they are just different applications of a few core principles and their enhancements. My key message is; whatever working approach you are faced with, such as Agile, remember the core principles described above. Let’s then dig deeper into Agile. In 2001 a group of authors wrote what we call Agile Manifesto. There has been numerous discussions around why this was written, but from my perspective it relates to the overall field of IT. You might have heard about the dot-com bubble. Before the breakthrough of milleania, the development of Internet caused that there were numerous companies basing their business on the technology of the Internet. Unfortunately, many of them did not do too good of a job and were overvalued. This bubble bursted around the breakthrough of millenia and meant a lot of negative implications for IT industry as a whole. It is a bit tricky to backtrace who was to blame for this, whether IT developers, business representatives or venture capitalists. As you are now reading this book to become a member of IT community, I will though bring a short story from this period for your learning. There were cases within this era, when IT developers were charging their companies heavy invoices for adjusting their calendar system. Supposedly, millenia change from 1900s to 2000s was a huge issue for company calendar systems and this needed to be worked on seriously. Of course there was nothing to work on and these greedy developers just wanted to earn on opportunity. As you can see, if there were multiple instances of such behaviour within a company, it is a clear indication of why the bubble developed and bursted. Learning for you; never bulls_it your customers. So, why was Agile Manifesto written just after the dot-com bubble bursted? In my view, it has something to do with trust. The IT sector as a whole needed to regain trust from investors, such as venture capitalists after they blew a lot of money once the bubble bursted and companies lost billions in valuations. One attempt of how to regain trust was to develop a new business model called Software-as-a-Service (SaaS), where software would no longer be bought, but rented by customers on monthly basis. You have a subscription on Google Drive or Dropbox? Welcome to this model. If you are interested in this, one of the early representatives of this field was Salesforce, which is successful until this day. Coming up with a new business model was not enough though. IT representatives needed to show that they learned, how software should not be developed and then proposed how it should be. And there you have it, Agile Manifesto. If you will decide to read Agile Manifesto, you will notice a lot more principles than the ones which I described above. Just to show you some: Individuals and Interactions over processes and tools Working Software over comprehensive documentation Customer Collaboration over contract negotiation Responding to Change over following a plan One thing will strike you though - Data Science is not a software development, or is it? Certainly, Data Science as a field was not around when Agile Manifesto was written, so even if Agile Manifesto is great, it certainly was not developed for Data Science purposes. 5.4 Legal regulations As you will on this level it will already be expected that you work with a data, it is necessary that you learn a bit about regulations. As I presume the readers of this book are located within Europe, I will talk about General Data Protection Regulation (GDPR) (if you are from outside, hoooray, the book made it really far!). 5.5 Producing valuable output You will start working on your new project (possibly job), doing your tasks and even collaborating well with the others, in iterative manner. Now the important thing will be to make sure that you are producing output which is valuable for your employer. I would advice you to focus on two words, while you can pick one which seems more reasonable for you. Firstly, focus on change. After you did your tasks, did something change? Now this change could be anything. The most obvious change comes in a matter of code, you created a script which does a job that was not in place before. Change however can be also in other areas - you discussed with business stakeholders within a project, and what changed is the level of information which you have about desired output. Secondly, you might rather focus on creation. There might be a lot of mess and noise around you. However as long as you make sure that your work is creating something, you are doing just fine. Focus on creation. Creation can be anything. Is Data Science software development? Yes and no. In my perspective there is a lot of things which we can learn from software development as Data Scientist (that is under yes answer). Drawing inspiritons from coding styles, documentation and comunication styles of successful software developer can really benefit Data Scientist. On the other hand, to my feeling Data Science needs to be significantly closer to context, as compared to software development (that is under no answer). If you are a backend developer of mobile application, you do not need to spend so much time with a customer or a user as a Data Scientist needs. Keep this in mind. Even though Data Science has a lot to learn from Software Development, it is a different field, which requires closer connection to context, customer or user. Actually, that is one of the reasons why Gartner in 2016 defined a Citizen Data Scientist, and also why I am now writing this book to help you become one. Though, Data Scientists need to find their own path. 5.6 Documenting your work JIRA "],
["contributor-II.html", "Chapter 6 Contributor: Part II 6.1 Cloud 6.2 Databases and Data Lakes 6.3 SQL 6.4 Programming 6.5 Data Preprocessing 6.6 Data Visualisation 6.7 Statistics", " Chapter 6 Contributor: Part II 6.1 Cloud What is cloud? Will I use cloud as a data scientist? How do I work on cloud? 6.2 Databases and Data Lakes What is a database? What is a data lake? 6.3 SQL What is SQL? How do I download entire table using SQL? How do I select columns using SQL? How do I filter rows using SQL? How do I join tables in SQL? 6.4 Programming How do I load data to my IDE? What types of features do we know? What is a function? How do I perform numeric operations in my IDE? 6.5 Data Preprocessing What are feature types? Numbr of unique values, compared to feature type. Think about which feature contains more information - numeric does. Ordered categorical does not contain distances. How do I describe data? For both types of features, describe what we observe such as mean or median. Percentiles. What is data aggregation? What are missing values? What are extreme values? The abnormal can be just a matter of perspective. What is data validation? 6.6 Data Visualisation Why do we need visualisations? Hans Rosling, one of the greatest visualisers of data uses visualisation to break down misconceptions, and also show the message in his data to anyone. How do I visualise numeric features? How do I visualise categorical features? How do I combine visualisation of two features? 6.7 Statistics What is normal distribution? What are independent features and dependent feature? What do I do with improperly distributet regression dependent feature? What do I do with imbalanced classification dependent feature? What is a hypothesis? What is a cost function? What is simplest regression model? What is simplest classification model? "],
["contributor-III.html", "Chapter 7 Contributor: Part I 7.1 Collaboration &amp; Git 7.2 Learning", " Chapter 7 Contributor: Part I 7.1 Collaboration &amp; Git How do I collaborate? What is code versioning? How do you use Git? 7.2 Learning What is Kaggle? What learning sources are there? Most common errors. How do I “Google” for debugging? Looking for a job/project "],
["statistician.html", "Chapter 8 Statistician 8.1 Pipeline 8.2 Productive Frameworks 8.3 Decision Tree 8.4 Bagging, Boosting &amp; Stacking 8.5 Unsupervised learning - Clustering 8.6 Unsupervised learning - Data Reduction 8.7 Feature Selection", " Chapter 8 Statistician 8.1 Pipeline What is Data Science pipeline? Data Science pipeline is simply a set of all modules, which add up to a Data Science model. Unfortunately, there is no unified view on this, as majority of Data Scientists approach their problems differently, as these are of different nature. I can put together a simpler one, as you will most likely start working on simpler datasets. What is a baseline model? Baseline model is the simplest solution to your Data Science hypothesis. Before I get to show you how a baseline model might look like, let me first describe you the logic why baseline model is crucial for every (Citizen) Data Scientist. Firstly, we already mentioned on level Observer that we cannot be sure whether Machine Learning will work. Secondly, we need to know whether when we do some improvement, if it had a positive impact on our predictive power. Thirdly, we always need to know a way forward. What is resampling? Putting aside a validation set and using it to evaluate model’s performance is already a sound practice. More common is for a Data Scientist to use resampling or cross-validation. When we put aside validation data, we can be unfortunate and put aside a piece of data which is not representative of the overall population. Due to that, we repeat the process of putting aside certain percentage (for instance 20%) of our training data several times (for instance 5 times), while the data we put aside at every run will be different. Here is how it might look like. insert picture of cross-validation We do same procedure as before, just that this time we are going to receive 5 figures of accuracy of our model on validation data. Usually it is the case that model performs more or less similarly on all 5 sets. If it is not the case, you should consult with a Senior Data Scientist who will advice you about the steps (it is out of our scope on this level and usually dataset dependent). The only thing that you need to do now is to average these accuracies and you get even more robust view on the performance of your model. Great! What is factor encoding? Most likely you remember that in first chapters of this book we talked about how our computers (and computing engines) view our data. Deep down all data are binary encoded into 1s and 0s, but from our point of view we have various forms of data that we would like to use. The most common data type that we would like to use (and can be problematic for our computer) are categorical features. Luckily for us, there are tons of frameworks and methods how we can properly represent a target feature to our algorithms. This is a very relevant practice. If you look at a framework called scikit-learn under Python, which is by many considered industry standard, it does not allow for categorical features which are not encoded. Give visual example The simplest method for factor encoding is so called one-hot encoding. We create a set of binary features (these are features which only contain two levels, such as 1s and 0s) out of our categorical feature (which can contain limitless number of levels). Here is how it looks. Give visual example of one-hot encoding What is high cardinality? Linear correlation What is feature importance? As soon as you fit your first Machine Learning model and would like to use it in your company/project, people will ask - “Ok so what are the most important features for the model?”. Not only we as humans have a natural curiosity, which makes us ask these kinds of questions, but it might be also beneficial for the project. What do we mean by feature importance? Even though we use 100 features in our model, it might happen that each one of them has different importance to our model. Let’s say you would like to predict how many cups of coffee I drink every day. Here are features which you have available in your independent set: Weight Height Hours of sleep Hours at work Brand of clothes Number of meals per day Maritial status Without using a model, let’s think about how important these features can be for predicting my coffee consumption. Certainly, the more hours I work and the less hours I sleep, the more coffee I am going to consume. I might be the person who likes to drink a coffee after a meal, so number of meals per day can give us some sort of indication. Also, if I am higher and weight more, I might need more coffeine to feel refreshed, so these two features might also hold some information. I cannot imagine how the brand of clothes or maritial status can influence my coffee consumption. Important: Hours of sleep Hours at work Might be important/influential: Number of meals per day Weight Height Most likely unimportant: Maritial status Brand of clothes The process through which we went, will be also a process through which your Machine Learning will go. It will use features which appear to be important and disregard the unimportant features. How do I know which features were important for my model? Look at the output of your linear or logistic regression. 8.2 Productive Frameworks What are productive frameworks? I will start off with a disclaimer that the term productive frameworks is most likely subjective to myself, so searching the internet for it might not be helpful. What I refer to when I say productive framework are Data Science frameworks which allow us to be, surprise - productive. There is a lot of them, in most of the languages related to Data Scince, but I will focus on several in this chapter: mlr mlrCPO recipes What is code reproducability? Onebig reason why productive frameworks allow us to be productive, is that when we write code in them, they often times handle an issue of code reproducability for us. What is this issue about? It is about ensuring that whenever new data come to our Data Science pipeline, the code will do exactly what we expect it to do. Let’s now write a simple code which fulfills all of the requirements that we learned so far. put a simple train example What do we do now, if we want to deploy this model into production? The script which we just wrote, we will refer to as train script and we need to write also a second script called predict script. I think you already guessed it, the predict script will not train the model anymore, only make predictions for the unlabelled data. Now the question for us is, how easy it is for us to construct a predict script? One crucial point at this stage will be to save states. States refer to anything that we need in order to reproduce a particular step in our pipeline. 8.3 Decision Tree What are tree based models? Tree based models are a new family of models on which we are going to expand our Machine Learning toolset. So far, we only relied on Linear Regression for regression problems and Logistic Regression for clasification problems. The downside of these models is that they are linear in nature while tree based models will allow us to work with non-linear problems usually better. What is a decision tree model? If you have ever reasoned about something, you have most likely constructed a decision tree model in your mind. Let’s say it is the last day before the birthday of your relative, which you of course forgot about and the small festive to celebrate the birthday is held tomorrow. You have no gift, while you would like to have some. Now comes the reasoning, as you arrived from work at 18:00 and the shops are closing soon, will you go out and buy the gift? Is there some bookshop opened in the vicinity of 2 km? If no, you will certainly not go, you are too tired. If yes, let’s consider further. Is it raining outside? If no, you will go. If yes, you reason further. Do you have a decent raincoat? If yes, you will go. If not, you will not go. This small reasoning which we just did was a construction of a decision tree, here it is: insert an example of decision tree Now if we come back to Machine Learning, we can reuse the same concept which we just did to make classification model. When we will be in the process of training a decision tree, the tree will be trying to find these paths and rules based on which it should reason. How will it do though? It will always search for the best split in our data. Let’s look at it. insert alternative view on decision tree pictures Let’s now get to the fitting of a decision tree. put code example 8.4 Bagging, Boosting &amp; Stacking What is bagging of ML models? We now know how to construct following models: Linear Regression Logistic Regression (classification) Decision Tree (regression or classification) We continue expanding our Machine Learning knowledge not by learning new sets of models, but by combining our current models to make one more powerful model. The first way of combining models is through bagging and I will also explain why combining of models is powerful. Let’s assume you would like to predict what weather will be tomorrow in Vienna (without looking at weather prediction services). What you can do is that you make a guess. You are going to do a reasoning based on your experience: It is mid of December so it will be fairly cold. You visited Vienna two years ago in November and it was around 5 degrees Celsius, so December will be colder than that. You heard that Europe is sunny these days. Based on your experience above, you will make a guess of 2 degrees Celsius. In this case you acted like single decision tree. How to make the prediction more accurate? By asking more people. The reason why this will make our prediction more accurate is that everyone has slightly different bacground and experience and when you average the predictions made by many people, it will most likely be more accurate than your sole prediction. Mention also averaging models when we use cross-validation. What is random forest? The above example of averaging people’s prediction about a weather is an example of a random forest. What is boosting of ML models? What is XGboost? What is stacking of ML models? 8.5 Unsupervised learning - Clustering What is unsupervised learning? Within unsupervised learning, we work with the very same data as in supervised learning, with only one difference - there is no label (target feature). Imagine a classroom, whereas your task is to analyse performance of students. Suppose that you obtain from teachers grades of the students. These will act as your labels, thus you can use a supervised approach where your target feature are your afformentioned grades. As for your independent features, you will colelct everything from students’ height, through how many hours they sleep, up until what they eat for brakfast. You fit a model exactly as we learned, such as linear regression and observe which features are the most impactful. Supervised approach can certainly work. Figure 8.1: Analysing student performance. Picture by Pixabay. What if, you do not have grades of students. Not only that, but you are unable to obtain from teachers, or students any feature that could represent how performant they are. What you do, to fulfill your task, is that you will apply unsupervised learning. You will try to group students with similar traits together - for instance you put together all students who eat good breakfast and sleep well. You come to a conclusion that approximately 10% of students do this. After grouping, based on these two characteristics, you find out that these students also behave well in the classroom and are enrolled for extracurriculum activities. You are not sure whether these students are performant but you managed to separate a group which posesses similar traits whereas these make sense with respect to the target of your exercise. This was an example of unsupervised learning, through clustering. What is clustering? Clustering is a field within unsupervised learning, which deals with creation of clusters. Clusters are simply groups of observations which posess similar traits, or characteristics. Under supervised learning, we had algorithms such as Linear or Logistic Regression. Here the story will be similar, we will learn algorithms such as K-Means clustering or Hierarchical Clustering. There is no one-rules them all algorithm. Though there are some basic algorithms, and more advanced ones. Look at the following picture developed by scikit-learn. Rows represent various kinds of data formats we might be facing. These data formats are purposely set up the way that human eye can right away see what is ideal grouping into clusters: 2 clusters: Inner circle, outer circle. 2 clusters: Left shape, right shape. 3 clusters: Small shapes on left and right, large in middle. 3 clusters: The three lines. 3 clusters: The three circles. 1 cluster: There is no obvious separation. Columns represent algorithms which are available to us to cluster the data correctly. I would like to point out three, which you will learn in this book: Column 1: This is a form of K-Means clustering. Column 5: This is a form of hierarchical clustering. Column 7: This is an advanced method of clustering, called DBSCAN. Figure 8.2: Comparison of clustering algorithms on various dataset forms. Picture by scikit-learn. What is K-Means clustering? K-Means is probably the simplest form of clustering. To demonstrate it properly (and to also compare it with other methods), we will use iris dataset. This is a famous dataset, as it is great for explaining concepts. The dataset looks like follows. We have hence four measures of the flower, and various kinds of flowes. Just to be sure, this is how you recognise Petal and Sepal: Figure 8.3: Comparison of clustering algorithms on various dataset forms. Picture by scikit-learn. Let’s now take two out of these four features - Sepal Width and Petal Width. When we plot all observed flowers and using a colour we also highlight which Specie they are, it is clear that the flowers can be recognised by the two characteristics which we picked. Can k-means clustering find the same groups within the data so that we would be able to recognise the Species? K-means clustering looks at your data from and drop at them K-number of points, for example 3 points. These points are dropped randomly into the space. What is hierarchical clustering? Why is distance so important? What are common applications of clustering? Marketing : It can be used to characterize &amp; discover customer segments for marketing purposes. Biology : It can be used for classification among different species of plants and animals. Libraries : It is used in clustering different books on the basis of topics and information. Insurance : It is used to acknowledge the customers, their policies and identifying the frauds. City Planning : It is used to make groups of houses and to study their values based on their geographical locations and other factors present. Earthquake studies : By learning the earthquake affected areas we can determine the dangerous zones. https://www.guru99.com/unsupervised-machine-learning.html 8.6 Unsupervised learning - Data Reduction What is data reduction? 8.7 Feature Selection Why don’t we use all features? What features can I always get rid of? Do we need to manually select features? What is regularization? What if I want to use model that does not select features? "],
["project-responsible.html", "Chapter 9 Project Responsible 9.1 Data Science project as product 9.2 Statistics 9.3 Data Engineering", " Chapter 9 Project Responsible Welcome to the level of Project Responsible. This level will broaden your mind mainly in softer areas - I guess it will be well appreciated change after the last chapter full of statistics. What is the most crucial here, is that you learn to see Data Science project as a product. This product should have customers,costs, it will have a lifecycle and everything else connected to it. Also, you will need to be able to establish a team of people who will develop this product. In the later part of this level we will come back to statistics and programming though. We need to expand your knowledge on advanced topics, such as local interpretations of a model. The reason for this is that even though you will not become an expert on these, it is neccessary to be aware of these if you want to develop Data Science project as a holistic product. Once the baseline is developed, making your product more profitable, might require these and hence you will be able to grasp them. 9.1 Data Science project as product Even without realising, all of us have already developed a product in one way or another. We have been contributing to some product in our professional life, we might have had some extra curriculum project. Maybe it was something very simple, such as cooking for our relatives a festive dinner - that is also a product which will have customers and will be evaluated and used. It is though something different to be responsible for a product and that is why we will have upcoming chapters. The key message from my perspective will be to explain to you adaptive development, so let’s go straight to it. What is an adaptive development? I will start this section with a very simple definition: Your Data Science project needs to be adaptive and customer/user need oriented. The upcoming sections will be elaborating on this definition, as I believe that in order to lead a successful project and define an efficient team to execute it, the definition shall be followed. I am pretty sure that you already heard the term adaptive multiple times, most likely with the context of globalization. With the development of Internet technologies, transportation capabilities and internationalization of business, things are simply faster then they used to be. I think that this links back to the definition of capitalism itself, whereas for this system to work properly, every year we need to achieve growth in what we produce. One sure way how to achieve such growth, is through becoming more efficient. For many years now, as it seems, to become more efficient can be achieved through being adaptive. As everything around us changes in order to be more effective, we need to be adaptive to these changes as well, in order to be even more effective (and successful), than the others. It’s a bit crazy thinking, but I believe this is a very simplified version of what is happening. It has it’s justification though - even for you as you aspire to be responsible for a Data Science project. Most likely, you have heard about Netflix and most likely also watched a movie or series through this service. If you have not, I recommend subscribing to their service and trying it out before you read upcoming lines. By talking about the history of Netflix, I would like to show you, what a definition of adaptive means. I would like you to draw inspiration from it, as successful Data Science is and also will be adapting as rapidly as Netflix needed to adapt in the past. Figure 9.1: Netflix is a clear showcase of adaptive development. Photo by John-Mark Smith from Pexels. See at: https://www.pexels.com/@jmark?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels The story starts at 1997 when the standard of watching movies was through VHS. These were pieces of plastic, which allowed you to watch a movie recorded on them in a cumbersome way. If Netflix wanted to be a traditional movie rental service, they would have taken VHS technology and launched an a brick store to rent them or sell them. They rather focused on customer need which was - laziness to go to brick stores to rent a movie every time, and eagerness to watch a lot of them at the comfort of home. What turned our as an ideal solution was to be renting through postal service. You would order a movie and it would be waiting in your post box when you arrive from work. VHS technology was too fragile for transport and expensive to operate. Thus, they decided to go for DVD format. This worked and people liked the service. You picked a movie online, ordered a DVD copy, paid for it a few dollars and it arrived to your post box. First successful adaptation to customer need done. Second one came shortly after, when Netflix allowed customers to be paying flat fee, a membership fee, on a monthly basis as compared to a pay-by-movie format. This turns our to be another successful adaptation as users like it until this day. On the other hand, you have companies unable to realize it even 20 years after Netflix did. Head over to YouTube and find some movie, you will see that it is offered on pay-by-movie basis. The use of DVDs was still a hastle though for their customers and Netflix was realising it. As the technology of Internet already developed decent bandwidths, which would allow for reasonable download speed of movies, Netflix thought of new format. They developed a piece of hardware called Netflix box. The idea was that in the evening you would choose a movie, the box would download it for you and next evening you can watch it. This would remove the needs for a DVD, while the service would remain having similar traits. The development of this took several years until 2005, when something crucial happened. Here, Netflix showed largest adaptation in its history. Netflix representatives observed the success of YouTube, which was a different form of video content delivery to the customer - through streaming. Netflix decided to scrape its idea of box completely and rather work on delivering content through streaming. It sounds insane even when I think about it today - scraping a technology tha you worked on and switching to something new from a scratch. Final piece of visible adaptation comes in 2006. Netflix decided to focus on a customer need of now knowing what to watch. The offer of movies was becoming so large, that we would get discouraged to watch a movie during the lengthy process of picking. They launched a public Machine Learning competition to develop a recommendation algorithm for their service. I hope that the story of a Netflix will inspire you in your Data Science work as Project Responsible. As you see, truthfull capability to adapt and adjust to customer needs might be way more difficult and require significantly larger out-of-the-box thinking than you originally thought. It does not matter in which context you are aspiring to become a Citizen Data Scientist, your projects will need to help your company in the strive of becoming more adaptive. Sometimes, this will require a lot of crazy thinking. How do I identify what customer/user needs? Here comes the trick; you should (almost) never ask your customer what he/she wants, you should rather find out. Do you remember how you in the level Observer learned about the biases in self-reported data? The same principles applies also here - even though people will always tell you what they want, it most likely will not be the same thing as they would use and benefit from. Let’s tak an example from applying Data Science in Marketing. Propensity models are one of the most applied concepts, regardless of an industry. Your company has a customer base (of thousands or millions of customers) and it also has several products (for example you sell various travel packages as a travel agency). You are hired because the company believes that through the use of data, the set of travel packages can be sold more to your customer base. First thing that you need to do is to realise that you have user and customer. The user is a person responsible for campaigns within a marketing department. This person will use your product and launch campaigns on it. Customers, are your actual customers of a company. Your question is becoming hence even more complex - what does your customer need and what does your user need? How to organize a team? Give two examples of how we were organized. One possibility was to have close involvement of Data Engineers, another was a possibility to have closely business representatives. How to lead a project? How to estimate timeline and budget for your project? When to apply which tools? What is adaptive transformation? By being product centric, you really are customer centric. How to assemble a team for data science project? How to find opportunities for a project? How to set a timeline for a project? Maybe something from marketing by Seth? Maybe soemthing from Sinek regarding Why? The project is really trying to reveal why something is happening often times? 9.2 Statistics Feature importance in detail What are Local Interpretations? AutoML Pipeline Timeseries Non-stationary variables Support Vector Machines Feature scaling 9.3 Data Engineering "],
["butter-knife.html", "Chapter 10 Butter Knife 10.1 Soft Skills 10.2 Algorithmic feature generation 10.3 Programming skills", " Chapter 10 Butter Knife 10.1 Soft Skills How to find good candidates? - personalities of a data scientist How to evaluate external vendors? Personality of a Data Scientist How to train data scientists? How to spread data science in a company? Why to contribute to open source communities? 10.2 Algorithmic feature generation Advanced Dimensionality Reduction Basics of Image Processing What is convolution Basics of Natural Language processing Basics of Data Engineering Recommender systems 10.3 Programming skills Using GPU for computing Dockerization and deployment Hyperparameter tuning Writing a package Featuretools Nested cross validation "],
["uncategorized-topics.html", "Chapter 11 Uncategorized topics", " Chapter 11 Uncategorized topics This chapter contains a list of topics which I consider interesting but I am unable to currently categorize them anywhere. Spark Bayesian methods Text processing Topic modelling Shell and Bash? Be Curious You are already at the level where curiosity will be what keeps you growing. With this chapter we are starting with a new practice that I will be leaving recommendations for additional sources that you can read Is Machine Learning Unethical or Racist? Are data scientists evil? Is there a reason why to be scared of these stalkers? Propensity models Can two models and their respective probabilities be really compared? Customer and user You need to distinguish between the two The falacy of absolute numbers For many years what matters a lot is a conversion rate or a success rate. I recommend though to be careful with it. Should I focus on how my code looks? I have a saying: “People have feelings. They feel when you code like sh_t.” Long and wide format of data A journalist asks a programmer: What makes a code bad? Programmer: No comment Parallel programming Self-organizing maps How does my PC view features? Numerical and categorical features. This needs to come before Supervised Learning as I am explaning classification and regression. No it doesn’t, it is explained only conceptually. (#fig:machine learning)Posted by u/victor_stefan to r/ProgrammerHumor, Reddit. What is information leakage? One of the steps we need to ensure is to prevent information leakage. This happens in cases, where our independent features contain some leaked (or spoiler) information about our target feature. Let me give you an example. What is correlation and causality? Make an example with babies and birds. Jobs which might talk about you I would say that positions mentioning statistics, causal inference, experimentation or A/B testing, incrementality measurement, segmentation, decision science, economics, and “analytics” generally will probably be more of that flavor. Emphasis on communication skills and visualization will be a component to these roles. You probably want to avoid applying to positions with a focus on engineering, architecture, or those specifically calling out deep learning or computer vision applications. I’d say a solid 50 percent of DS jobs want an insight specialist over a ML specialist regardless of the job posting. Skillset wrong view Machine Learning Areas: Supervised Machine Learning (49%) Unsupervised Learning (26%) Time Series (25%) Natural Language Processing (19%) Outlier detection (16%) Computer Vision (15%) Recommendation Engines (14%) Survival Analysis (8%) Reinforcement Learning (6%) Adversarial Learning (4%) Machine Learning Techniques: Logistic Regression (54%) Decision Trees - Random Forests (43%) Support Vector Machines (32%) Decision Trees - Gradient Boosted Machines (31%) Bayesian Techniques (27%) Neural Networks - CNNs (26%) Ensemble Methods (22%) Gradient Boosting (17%) Neural Networks - RNNs (15%) Hidden Markov Models HMMs (9%) Current hiring I saw an article somewhere the other day that suggested that even really good Machine Learning candidates were failing to get good jobs. Apparently the universities are now churning out many ML graduates … but the jobs are surprisingly not there. The article suggested that the top users of AI have decided to build very small teams of stellar AI gurus : geniuses with multiple PhDs. The firms do NOT want hoards of simply ‘very good’ ML candidates. Is this likely to be true? If so, is it just the top firms which are this fussy? In which case, are there other firms who are happy to hire the ‘good’ ML candidates? What is so exciting about this profession? The endlessness of the game is my favourite part. I like always having new things to learn. Hard to imagine ever becoming bored. More on definition of the field correct in the fact that analytics has been around a long time. Only recently has it graduated into the realm of BI. I’ll preface everything here with this: EVERY industry/company/department has slightly different needs and scope so YMMV. Within BI there are three main disciplines: Data Engineer, Data Analyst, Data Scientist. Data Engineers focus on the data at point of capture, storage architecture, and access. This involves multiple parties too if the enterprise is large/complex enough; DB architects, DB Admins, Security and so on. In essence the DE focuses on making the data accessible down the data pipeline. Data Analyst uses both prescriptive analytics, the use of historical data to detail where you and and where you’ve been, and predictive analytics, the use of that same historical data to look for how changes in business can effect whatever business KPI/SLA/Metric they are trying to improve. Data Sciencist uses that same historical data but in a more theory approach. They commonly are looking for new and unforeseen instances. They also look more at academic approaches; automation of predictive models to allow for streamlined processes (see digital twinning in manufacturing), use of AI/ML to limit need for human intervention (see neural nets deployed as “Brokers” in insurance), or NLP/sentiment analysis (see Alexa/OK Google or websites like booking.com) A journeyman DS is not so different from a DA. Both are using the tools and techniques at their disposal to drive data driven decisions within their company/organization. One may rely more on BI tools like Alteryx/Tableau, the other may focus more on statistical programming like Python and R. In the end they are all part of the same ecosystem. A DA may focus more on the business processes (a SME for the business) while a DS may look at things more clinically. To get back to your question on what “pure” data scientist does? That is harder to quantify. There are Data Scientists that focus purely on model development. Only looking at statistical data sets to derive better/fast models in ML/AI. They hardly ever work on real world material and drive a business decision. They are also like any other theory driven scientist; a very small sub-class of the whole. Interview questions https://old.reddit.com/r/datascience/comments/edhzdg/d_data_science_interview_questions_mega_thread/ What is right and wrong Thinking back to my days as a first year data scientist, one of the most difficult transitions I’ve seen people make is how they measure their value. Because academia is primarily an environment in which you’re measured by how right or wrong you are, a lot of people transition into the workplace thinking the same. What’s worse, some go further and extend that to the point of thinking that there is value in proving others wrong. That is fundamentally not going to work. And that is because people in the workplace are measured almost exclusively on how productive they are - they are measured on results. Corollary 1: if it’s wrong but it works, then it’s not wrong. Corollary 2: if you’re right but it doesn’t change the outcome, then it doesn’t matter. Corollary 3: if you’re right, but it doesn’t work, then you’re wrong. Corollary 4: if you prove someone else wrong, but their answer works and yours doesn’t, then they’re right and you’re wrong. Corollary 5: if you prove someone’s solution to be wrong even though it does provide value, then you have not yet provided any value until you propose something better. I cannot emphasize how much you can limit your career by focusing on right vs. wrong. Right vs. wrong is irrelevant; productivity always rules. EDIT: Since many have had an issue with the definition of something that works vs. something that is wrong: This is the part that people miss - it is rare that bad science works. When things that a person sees as “wrong science” work, I normally find that the overwhelming majority of the time, if that person is junior, what is actually happening is that: It’s not actually wrong, and the person just doesn’t understand why it’s right. It’s not 100% right, but it’s right enough to provide value. And some people interpret that to mean wrong, which is too binary in the world of modeling. 95% right isn’t wrong, it’s just 95% right. The only scenario where you will see bad science work with any degree of frequency is when it has been tested over too limited a set of scenarios - in which case it should be relatively easy to point out where it will fail, and then you can focus on outputs - on how it won’t work, rather than on it being wrong. Excellent points! I’ll add that fancy novel techniques mean almost nothing to business folks. Simple and effective is just as good if not better. You can and will get out played by someone with a simple brute force technique that they implement quickly while you try and perfect yours. More on citizen data scientist Although the term has existed for a couple of years now you won’t find job listings for citizen data scientist on Glassdoor.com. That’s because it’s not a role that an organization is going to hire for, but more like a requirement, they need to fill. People in the industry are using the term citizen, but those doing the hiring are focused on the tasks not currently getting done but require attention. It’s common knowledge that we have a shortage of data scientists, and businesses realize that not every job function needs to be done by someone with an advanced degree as they reckon with this shortage. Instead, a citizen data scientist can be trained to do work that is needed but not currently getting done. Develop visualisation skills as a crucial skill Once you’ve got the foundation of big data skills, you need to develop visualization skills. Your employer or organization doesn’t care about a jumble of numbers on his or her desk. Your job is to present the data in a visually compelling way. Google Data Studio as a learning alternative to Power BI, Tableau or Clicksense Negative opinion What will not work is creating a partly automated software and expect people with little training or understanding of data to be able to use it in new situations. Imagine a combination of untrained pilot and an auto-pilot that works 90% of the time. The plane will fly fine for a while until an unpredictable situation will arise, and the autopilot will signal a problem and revert control to the untrained pilot. Bad things will happen. Similar logic is behind Google decision to remove a steering wheel from its self-driving cars - a driver who is relying on a car almost all the time is unlikely to react quickly and adequately in an emergency when the car cannot cope. I should state this at the beginning of the book that I do not intend readers to become partly trained. Should I rename the term? I run a small team of data analysts and I’m offended by the term “citizen data scientist” to describe us. We grew from the business developing data skills as we went along. So, we don’t have much formal training, but we can run circles around other analysts with all the proper credentials. Calling us citizen data scientists sounds very condescending. Like we’re still sitting at the kid’s table. Keep the terms separate. I acknowledge that we’re not “data scientists” by any formal definition, but we are skilled data analysts. Someday we may earn the coveted title of, “Data Scientist,” but until then don’t put me at the kid’s table. Steps in ML project https://www.lpsm.paris/pageperso/has/source/Hand-on-ML.pdf This is what Oreilly says, which I tend to disagree with. 1. Look at the big picture. 2. Get the data. 3. Discover and visualize the data to gain insights. 4. Prepare the data for Machine Learning algorithms. 5. Select a model and train it. 6. Fine-tune your model. 7. Present your solution. 8. Launch, monitor, and maintain your system I like how they showcase what a confusion matrix is. They also describe ROC curve Also Precision Recall tradeoff Linear regression is a copy from Andrew NG Personality of Data Scientist Get into flow "]
]
