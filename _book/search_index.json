[
["index.html", "Intuitive Data Science Chapter 1 Foreword", " Intuitive Data Science Robert Barcik 2019-12-22 Chapter 1 Foreword Data Science is undoubtedly one of the fastest growing areas within IT. This growth is not unjustified though. Majority of the firms did undergo a process of digitalization within the past 2-3 decades. Opportunities within digitalization, such as digital offering of products, digital channels for marketing, are reaching new maturity points. According to microeconomics and its law of diminishing marginal utility, for every penny invested early into an activity, the benefit on this penny will be large. While if we keep investing, after certain time the investment of same penny will yield only much smaller benefit. Due to this reason, lots of companies are these days turning from digitalization (as a sole investment) into a related field of data science. Figure 1.1: Display of Law of Diminishing Marginal Utility. The utility obtained from eating every next chocolate bar yields less utility, until the utility even starts to be negative. This is supported by the fact that the process of digitalization inevitably collects data. Thereafter companies see that even with minimal investment into this trend, the benefits can be significant. If you would look at the past major drivers which influenced companies, you could observe something like this. Figure 1.2: Technologies which are driving markets throughout recent history. And this is where you come in, with your opportunity to grow your career, naturally following what market is hungry for. "],
["being-aware.html", "Chapter 2 Being Aware 2.1 Five Cornerstones 2.2 Defining Data Science 2.3 Defining Data Scientist 2.4 Who is Citizen Data Scientist 2.5 Value of Citizen Data Scientist 2.6 Anyone Can Become Citizen Data Scientist 2.7 The 6 levels 2.8 Learning Without Math 2.9 Level 1 Reached", " Chapter 2 Being Aware 2.1 Five Cornerstones You will quickly notice that this book is structured quite differently compared to other offerings on the market. Let me reason about that, even before I introduce you to the structure and the transformation that the book offers. I have been a lecturer and trainer for a few years and I always notice that content does not matter as much as the path through the content as well as the assumptions upon which the content is built. Only if both of these are fine, the book/course/training can offer you (the reader/student) a transformation - taking you from where you are, to where you want to be. In this case I am hoping you would like to make a living within Data Science or create some kick-a__ project within it! This book is hence built on five cornerstones: Data Science is a practice of using data to create a direct benefit. Citizen Data Scientist is becoming a crucial occupation. Anyone can become a Citizen Data Scientist. There are 6 levels of Citizen Data Scientist and these should act as a learning curve. Math is not required as a teaching tool in order to train a Citizen Data Scientist and can be replaced by visualisation and intuition. Did I go crazy claiming that anyone can become a Data Scientist? And what about the Math which is appraised by everyone as the crucial tool within data science?!? Maybe I have, but these cornerstones (assumptions) worked for me as well as for my students within the past years. Let me first explain why I believe that these things might work out for you. 2.2 Defining Data Science The very first thing that I would like to do is to answer a simple question: What is data and what is information?. The difference between the two is in their value. Data inherently do not have any value for their owner, simply because he/she cannot do anything beneficial upon them. On the other hand information can be valuable for its owner. These two are however thinnly connected - as information is derived from data, while this process is called Data Science. I will give you an example from my recent past on how I got a value out of Data Science. I was always interested in my sleep, as I know that in order to live a happy life, I shall sleep well. The only two things I know really is how long I sleep and how do I feel in the morning. I then found an app on my phone which was promising to analyze my sleep and give me recommendations on how to improve my sleep. The app is using accelerometer in my phone, recording my movements at night. During the first two nights, the app only collected data - every morning I was able to see various graphs on how I slept. After a week though, recommendations came such as “”hen you go to bed late, around 23:00, even if you sleep full 8 hours, your sleep quality suffers.“. This is already information which had a value for me. The app is Data Science process of creating valuable informaton from data. If you now turn to search engines and start to search for Data Science terms, you will be overwhelmed by articles and (unfortunately) buzzwords - Machine Learning, Data Preprocessing, Artificiall Intelligence, Tensorflow, R, Python. These are at the end only methods and tools of Data Science to achieve a result which we described above. This might actually be the most important message from this book (that is why it is in the beginning) - always keep in mind that the methods should always serve the purpose which Data Science has. 2.3 Defining Data Scientist Now that we know what Data Science is, it is quite easy to define who a Data Scientist is. He/She is a practitioner of this field, who uses its methods to achieve its goal. Usually, Data Scientist has general education in three area - Programming, Mathematics and Statistics. Notice that none of these areas deals with context or domain such as Banking, Medicine or Engineering. Thus Data Scientist - by definition, should be domain-agnostic. He/she should be able to join any of mentioned domains, and many more, apply the methods of Data Science to fulfill the goal of creating value, through information, out of data. This is very important point for you as you will now see. 2.4 Who is Citizen Data Scientist Now that you heard about Data Scienstits, who are Citizen Data Scientists? I think they are the most crucial occupations in the future of the field of Data Science. You heard me right, not the original Data Scientists, but the new generation of Citizen Data Scientists. Let me show you why. Data Science as a field has been around for decades in one form or another. There has been market analysts, risk modellers, product developers, artificial intelligence developers and so on. If we however relate to the definition of Data Science above in a way that a Data Scientist can essentially come to any company and make use of its data to create benefit, we can go back to the millenia breakthrough. Universities start to offer programs with names like “Data Analysis”, “Business Intelligence” and first candidates are leaving these programs. It feels like the field is growing tremedously - growth in GPU powers, Deep Learning, Automated Machine Learning products. Especially since 2010s, job advertisments revolving around Data Science are exploding! Though is the field really growing from the perspective of market? I would say very slowly, simply because the created benefit for companies (and their customers) is growing slowly. This is cause by the fact that the field is still relying on the slow and linear growth of the first generation of Data Scientists - ones having formal education, being able to guarantee the work. Now two scenarios can occur when a company attempts to create benefit/profit through data science: 1. It hires actual Data Scientists, whereas costs to hire them are tremendous and what is even more expensive is to make them productive. Finding for them process which can be optimized through their methods and then properly optimizing it is lengthy and costly. As these are domain-agnostic, the company needs to also educate them and integrate them into contexts. Company slowly looses interest and stops believing that actual profit in a decent scale can be created this way. 2. It does not hire actual Data Scientists, as they are expensive and scarce. People who come to the company have been originally trained in other areas and are now expected to be actual Data Scientists. The company has just thrown away a lot of their potential as only their recently acquired experience are counted and not their original context. The hirees are now pushed equally hard as the actual Data Scientists and are struggling to create valuable products. Company slowly looses interest and stops believing that actual profit in decent scale can be created this way. You might now disagree with me and say I am too much of a negativist. To justify my thinking in your mind, let me ask you only one question. As employees, we are expected to create larger profit than what we cost our employer. How many data scientists you know, who can prove that they are repeatedly creating profit for their companies? Not so many right? Thereafter I believe that above described struggle holds. I believe that the answer to this struggle of many companies is recognition of a Citizen Data Scientist (the ones coming in point 2 above) and definition of their relationship to Data Scientists. The market is begging us to do so and we can see it by who comes to job interviews when we make an advertisment for a position of a Data Scientist. These are people who do not have formal education or direct experience. Let me tell you one story from my past. I worked in large organization with thousands of employees as a Data Scientist. Out o nowhere, I received an email from a colleague I never heard of before. She claimed that she is really passionate about the methods of predictive modelling and Data Science and is eager to apply these methods in her CRM department. We met and she showed me R-script in which she tried a bunch of Machine Learning algorithms and preprocessing methods. What I did in turn was that I encouraged her to continue with the passion because the field has a future and I advised her about further methods she can try. Only years later, I realized my mistakes in that meeting and that I met one of the first Citizen Data Scientists. I then learned that she left the company as she never managed to get her passion to productive work. I was one of the reasons why she churned because I gave her advice on how to become the first generation Data Scientist, not the second generation of Citizen Data Scientist. Let’s look at the definition by Gartner in 2016: Citizen data science bridges the gap between mainstream self-service data discovery by business users and the advanced analytics techniques of data scientists. This colleague of mine was originally in the first part of definition (business user), while I was in the second part of it (advanced analytics). She was trying to become a bridge between the two, without even realising it! Unfortunately I am only smarter years later and when the same situation happens these days, I react in a different way. Also, the company which I work for now is smarter and allows for an environment where Citizen Data Scientists are given space. She had something that I could never have - expertise in her CRM department and if I was about to give her advice now it would be: - Focus on how a predictive model will create benefit in your department. Unless you create a benefit, whatever we code is worthless. - Here is a (short) list of things you should try in your R-script (most of which you already have). You do not need to push for more complex methods and crazy good programming, what you have is already enough. If something more complex is required later on, I will do it for you. Ehm, exact opposite of what I originally did, right? Citizen Data Scientist therefore stays in his/her original deparment and context, and only o a required (limited) extend enhances her knowledge on Data Science, so that he/she can create the value. He/she posesses edge over (first generation) of Data Scientist in the context knowledge and can hence create more valuable products. 2.5 Value of Citizen Data Scientist As everyone says it these days - data are everywhere, companies just need to start utilizing them. We can hence take as granted that Data Science will have justification in upcoming years. Though why do companies need Citizen Data Scientists and why they should focus on them instead of the first generation of Data Scientists? Simply because the growth in created benefit will be much greater as there can be multiple of them compared to Data Scientists and they are much closer to the context. They know better what needs to be optimized and in which way, because the context (business area) is their origin. What happens with the original (first generation) Data Scientists? We will still need them, just in slightly different role as until now. They become heavy developers, guarantors and trainers. Whenever a complex problem occurs, which needs technically challenging solution, it will be them who focus on it. The thing is, that in many organizations, there is only a handful of such problems, that is why this makes sense. Secondly, they act as guarantors of solutions developed by Citizen Data Scientists. This again makes sense, because reading through something and commenting on it takes a fraction of time as compared to developing it. Finally, they should act as trainers as there is a need to train, coach and mentor a lot of Citizen Data Scientists. 2.6 Anyone Can Become Citizen Data Scientist I honestly believe that whomever you are, you can become a Citizen Data Scientist.Why? Because Data Science methods add up to only about half of the requirements for a success, the other half is context knowledge and application. Thereafter whatever your background is, you can become one and create value to your company. Let me list some examples: - You are a cashier at a grocery store. There are Data Science projects aimed at optimizing customer experience. Data Scientists without your help will never figure out …add - You are biomedical researcher add - You are branch advisor for a bank add 2.7 The 6 levels You can have a great content, great exercises, but unless I am able to keep you motivated during your jurney and you feel like you are growing with every module finished, my course/book is not good. That is why this book sets 6 levels through which you will be growing. With each new level, your power within data science will inherently grow and so will your value for employers. This gives you the ultimate opportunity to practice your skills - because you will be able to; in real world! As you will notice, each level holds its definition and a length required (from my perspective) to reach this level from a previous one. Being Aware You know what the field is about, what it can and cannot achieve. Learning: 1 hour Observer You are able to observe a data science project running, without being able to contribute to it. You are though able to make use of the outputs of a Data Science project as well as help with inputs for such project. Learning: 5 hours Contributor You are able to assume a (simple) task(s) within Data Science project, and create a benefit to the project productively. Learning: 30 hours Statistician You are able to assume any basic task within a Data Science project and hold responsibility for statistical parts. Learning: 80 hours Project Responsible You are able to define Data Science project and execute it. Learning: 40 hours Guru You are able to grow Data Science initiative in your organization both effectively (hiring) as well as conceptually (specializations). Learning: 40 hours Each of these levels is very different both from the content perspective, as well as from desirable approach by you. For example, on a Statistician level, you are going to be focused solely on statistical formulas and their intuition. Be prepared for sitting longer hours, stretching your analytical mind. All of a sudden, this stops and in order to become a Project Responsible, we are going to sharpen your soft skills, such as how to organize a team effectively. It will be needed to free your mind and think about people instead of code. What is the biggest struggle of anyone who would like to become a (Citizen) Data Scientist? To get real world experience and practice. Whatever people claim within their online courses and trainings, the trainer will never be able to offer you what is awaiting for you in the real world of Data Science. Even I don’t claim it about my trainings. The only truthful way is hence to go to a real project, that is why I created the levels, so that at each you have it as easy as possible to get to real world problems. Let me show you an example of why I decided to write and teach in this way. You come to a job interview (external or internal one) and say that you have been learning and would like to become part of this Data Science project. The conversation might go as follows: Interviewer: Great to see your interest. So how can you contribute to our projects and help out? You: Well, I learned a bit of Python, I am able to put together some basic statistical model and also do some data preprocessing. Let me stop here for a second and explain to you what is happening behind this scene of this situation, which is happening probably hundreds of times every day. You are having hard time selling yourself and interviewer is having hard time evaluating your qualities. You are both stuck, due to simple reason - you have listed skills instead of capability to contribute. Now let’s do a similar conversation and we will reuse one of the levels from above as your answer: Interviewer: Great to see your interest. So how can you contribute to our projects and help out? You: I am able to act as an “Observer” to your Data Science projects. This means that I cannot directly contribute to it’s productive code pipeline, but I am able to help out with inputs and outputs. Due to my extensive background in Retail Banking, I know what features might be interesting to collect about customers and used in models and also how to apply outputs of your project in Marketing Campaigns. Do you feel the difference between the two conversations? Instead of talking about skills, you started to talk about a possible contribution which you can do. Moreover, you had a space to relate to your previous field - we all have something where we are coming from. This is what really matters for Citizen Data Scientist - show possible contribution that will be impacting the project and benefit out of it directly. 2.8 Learning Without Math Mathematics is undoubtedly the foundation of many fields, Data Science being one of them. If one masters it, incredible beauties will uncover in a lot of mathematical formulae and deep understanding of many concepts can be achieved. It has only one problem…“it’s f_____g hard to learn!!!”. I personally know only two kinds of people. First group are ones who are comfortable with math - they liked the subject since many years and have the incredible patience to learn it. Most of the times they also invested into some form of formal education within math. The second group are people who don’t like it and whenever they meet it, they search for intuitions and workarounds just to get the piece of work done. The share of people who belong to the latter group among my friends is 95% and that could be a sad fact for a field like Data Science which would like to grow, while being based heavily on Math. Or is it? In my perspective, the first generation of Data Scientists are primarily the ones who are fine with map and the second generation belongs to the non-likers. Now let me tell you, it’s perfectly fine and natural to not like math and not be comfortable with it. Who should adjust? Should it be 95% of population or teaching methods of Data Science? Let me bet on the latter… That is why this book will not teach anything through mathematics. I honestly believe that in order to train a Citizen Data Scientist, math is not only un-needed but also un-recommended, based on my argumentation above. 2.9 Level 1 Reached Congratulations! Without even realizing it, you have reached the first level which I call “Being Aware”. You are now aware of what Data Science is, what is isn’t and what is the role of Citizen Data Scientist. "],
["observer.html", "Chapter 3 Observer 3.1 About this level 3.2 Be Skeptical About Data 3.3 Data Science Project 3.4 Your Contribution 3.5 Data and Computing 3.6 Using Data 3.7 Machine Learning 3.8 Supervised Learning", " Chapter 3 Observer 3.1 About this level Welcome to Level 2 - Observer. Once you reach this level, you will be able to act as an Observer of a Data Science project. You are not yet directly able to contribute to it (such as through coding), but you will be able to use outputs of it and help with the inputs. Thus if you are a campaigns manager in marketing department, you will know what to ask Data Scientists for and how can you use it. Also, you will have some basic tools with which you can evaluate whether the Data Scientist that your initiative is dependent on did a good job or not. As I mentioned, you will also be able to help out with inputs. Whether you know it or not - Data Science is dependent on people like you, who have the context knowledge. That is why there is the famous saying, “Garbage in, Gargage out”. If you will be able to point a Data Scientist to what he should use and what he should not use, he will be grateful (unless he is stubborn). We will need to develop your skills in three different areas: Soft understanding of Data Science Be Skeptical About Data Data Science Project Your Contribution Programming Data and Compuing Using Data Statistics Machine Learning Supervised Learning 3.2 Be Skeptical About Data Are data really powerful? Being skeptical about the power and bias in your data is the most crucial skill, or mindset which you will aquire on this level. Even experienced Data Scientists sometimes forget this point. Firstly, let’s think about the power of your data. It might be necessary to ask, whether the data even can answer the questions we intend to ask. Let’s say that you got a task of optimizing bus transportation in your city, fairly large city let’s say. The optimization should bring a benefit that the number of busses on each route should be adjusted to the demand, making passangers happier, thereafter using the busses more, earning you money and helping the environment. The dataset which you have available are: Every bus route and number of busses operating it. Every bus stop and a delay that every bus had on that particular stop. Every ticket bougth - at a bus, at a stop or online. Do you think that this dataset will be sufficient to fulfill the task? Figure 3.1: Display of Law of Diminishing Marginal Utility. The utility obtained from eating every next chocolate bar yields less utility, until the utility even starts to be negative. You do a perfect Data Science project, utilizing the dataset to its maximum. Some of the things you will do based on a dataset: Bus routes, where people buy most tickets on their stops will increase in number of buses operating them. You are hoping that this way, the busses which might be crowded will be more comfortable for the large number of people using this route. Bus routes, where buses tend to be late are rerouted. You found some alternative paths, alternative places for a bus stop and are hoping that this will decrease the delay which buses have, making people more happy. Seems reasonable right? There is though a catch and let’s see what might actually happen as a result of your Data Science project. What will most likely happen is that you might optimize for the opposite. You have increased the busses for people who only occassionally travel with a bus, such as pensioners when they go for a walk in city centre. You have not optimized for heavy users of bus transportation - commuters who daily commute to and from work. Similarly, your change in routes meets opposite of success. The reason why the buses are late on these routes is that these are the major paths for people in the city to use. Why have we failed when we dida perfect Data Science and our assumptions about the bus routes seemed to be very reasonable? The answer is that our data did not have the power to correctly solve the problem. What we mainly lacked was the online purchases of tickets as well as yearly subscriptions to public transportation services. Both of these ticket purchases are untracable. As a matter of fact, people who buy yearly subscriptions are the heaviest users of public transportation and we really have no data about their movement. Thus, our dataset simply did not have the required data. One way to solve this would be to change the system in itself. For instance, even if you have a yearly subscription or a ticket bought online, you have to tag it once you enter a bus. It won’t wihtdraw any money from your account, though your yearly ticket will be invalid, unless tagged. The message which I wanted to translate with this short example was not how to optimize public transportation, but s that you are skeptical about the power of your data. Are some data specifically biased? When it comes to data, there are two areas which I tend to distinguish: Sensory Data that relate solely to machinery or natural processes. You are a chemical engineer running trials and observing time for a certain reaciton to happen. You could be a mechanical engineer watching whether an error won’t occur in your machinery. Assuming that your sensor is reliable and well setup, your data should be unbiased as both machine and nature (in this sense) are stable in behavior. Human-related data that display human behavior. Self-reported data, such as surveys. Collected data, such as logs from mobile application. To anwer our question whether some data are specifically biased, simple answer is YES. There is a lot of data from my perspective which are just wrong to be used and you as a Citizen Data Scientist should not use. I do now wish to talk now about data quality issues, but about the nature of data, which is connected with how the data came into existence. There are a lot of datasets which came to existence by asking people regarding their state or opinion. Take elections for instance. If you look at US elections in 2016, for a very long time it was seemingly impossible that Trump will win the elections. Yet, he won, even though the data from polls told us otherwise. What happened is that this (poll) dataset was created by asking people, while unfortunately it is in human nature that: - people lie with purpose - people are lazy - people want to appear different than they are - people do not know themselves Thereafter, when your Data Science project is working with people, you should not rely on data which were created through asking. I will be honest, from my perspective all survey data are just wrong and in nowadays world, I would simply skip this practice as we have better means of data collection. Let’s assume you work in a bank and your task is to develop a Data Science model which will automatically be granting loans. As you are working within loans department, you very well know that the primary factor for whether your bank will grant a loan is the income of a customer. You also know that there is a field in your database that says “income” to each one of your customers. Problem solved right? You just collect a few more fields and create a model as you were asked. Well, I recommend something different. If you will ask around you will find out that this “income” field was created through self-reporting of your customers. When they are setting up their account, the advisor in a branch will ask them what is their income and they report figure X. Trust me when I say, that due to one of the reasons listed above, the customer will report a wrong number. Ok, then - you tell yourself that you are smarter than that and decide to observe customer’s incoming salary transactions which you have tagged in your system. You have overcome the problem of self-reported information, right? You deploy your model into production and run your automatic loaning system. What will happen is that in some time frauds will arise and there will be a lot of customers who took huge loans and do not pay them back. How could this happen? Well, people learned how to cheat your model for their own benefit. Fraudsters are now once a month sending each other transactions which are tagged by your system as salaries and thereafter your “income” field for this customer is huge. They are then automatically granted significant loans. So, how can we solve this? The key is to obtain information which is desired through data which were created by human, while he was doing something completely natural and at best, unrelated. You need to think about an action, or a process of a customer though which he/she unknowingly displays his/her truthful income. What about groceries? We all do grocery shopping, thereafter this might be a reliable source of information. What if you look at how much this customer is spending for groceries monthly and relate it to his/hers assumed income. If you have reported 1500 Euro income, how comes that you only spend 100 Euro for groceries? I would rather presume that your income is somewhere around 500 Euro. This is of course not a perfect solution, but what I am trying to translate is the skeptical thinking about how to solve the problem with bias about human-related data. 3.3 Data Science Project How does Data Science project create benefit? Let’s start from the very end, what should a project generate. It should be new information, in various form, that creates benefit for a person or company that financed this project. Let me emphasize the point of that the project creates new information, not new data. This is because data by themselves, have no benefit, and thereafter there would be no point in running a project that creates data and not information. This information can be in various forms, so let’s take a look at some examples: When I was at university I was tasked with creation of project, which would take recordings of people walking and it should create an information of whether people on the recordings are suffering from Parkinson’s disease or not. Having this information created various benefits, for instance (if working properly), expensive time of doctor is saved. Ultimately, people could self-administer this test at home without the need of visiting the doctor. The information created was the statement “Has Parkinson disease” or “Does not have Parkinson disease” to every recording. Once I ran a project which was supposed to determine a location of newly opened retail stores in a country. The input for my project was the network of roads and cities in the country with their respective counts of inhabitants. The information which my project was creating was a potential position of retail stores that would be optimized for a distribution of roads and where people live. This information created a clear benefit - competitive advantage for particular retailer. Numerous times in the past I was asked to determine customer’s probability to purchase an item. My project always took as an input various customer data such as their demographics while it predicted a probability to purchase particular item (information). This information has a benefit that a business representative can then decide whether he/she will invest into marketing for particular customer. I could go on, but I guess you already have the picture of what is the single requirement of any Data Science project - creating an information from which we benefit. The beneficial information can now really vary in forms. It can be anything as you already saw, but I would like to broaden your horizon even more. The output of a Data cience project can be a single diagram, or even a sentence (thereafter it does not have to be predictive modelling or optimization only). How does Data Science project look like? Essentially, all Data Science is about are hypothesis. This is how hypothesis is defined by Oxford dictionary: a supposition or proposed explanation made on the basis of limited evidence as a starting point for further investigation. You would be surprised by how many Data Scientists are forgetting some parts of this definition so I would like to focus on it. At the beginning of a project you suppose something can be done - “We are able to find ideal locations for our retail stores.”. Then you examine whether you already do not have enough evidence which would answer your supposition - “There is no other retailer similar to us in a given country and as we are new to the country we also have no knowledge.” Finally, as we already discussed, there is a clear benefit if our assumption is fulfilled - “Once ideal locations are found, our stores will be easily accessible by customers and this will be profitable”. Once you work out your hypothesis, your Data Science project can outset. You will seek for available data that you can use to assess your supposition. In later chapters, you will learn about concepts such as Minimum-Viable-Product or experimentation that will help you to set up as simple way as possible to assess your supposition. But, let’s leave it hear from now - remember to always set up a project based on your hypothesis. 3.4 Your Contribution How can I contribute to Data Science project? I have to make a small assumption here about who you are. You are someone who works in domain - for example banking, paper industry, insurance, car sales, anything. You either have formal education in this domain, or working experience. This means that you have something that no Data Scientist has - specific knowledge of the domain. Thereafter, as we already mentioned, you can contribute to the Data Science project in two ways already on this level: As Observer, contribute through inputs and through outputs, not into the central process. As you already know, the basis of every Data Science project are data (input). As you have domain knowledge, you might now about data that Data Scientist is not aware of. I will demonstrate this from one project of mine from the past. My project was using log data from a mobile application, whereas users had to go through several screens in order to complete purchase process. I collected all the log data and what it was showing was that a certain group of users was dropping of from the sales process at various point. I was struggling with it and I was unable to explain why this group of users (which had no clear characteristics) was dropping off at various times during the day. I was then approached by a call center representative, which for several years sells the same product over the phone. He did not even have to look at the data and gave me the answer! These were parents of young children! As the sales process takes an hour to get through all the screens, there is a solid probability that if you are a parent of a baby, the baby will start crying and you will of course leave your computer. The data would never reveal me this pattern, the domain knowledge did instantly. You can be the gentleman from call centre, which moves the project a light year ahead! Secondly, every data science project should produce an information (output) that will be used to create benefit. This component of the Data Science project is often times underappreciated and you can be the one who fixes it. I will now go back to my projects where I was supposed to predict the probability that customer purchases a product. I was given this task by our CRM department to optimize email campaigns - so that these are sent only to customers who have decent probability to buy the item. Everything worked well, the Data Science project ran through, the information was created. BUT, the created benefit was close to nothing. As it later turned out, customers are generally overfed with emails and digital channels. I then met with a representative from our branch segment and he said that this information could be of great use for our sellers. The sellers have only very limited time when the customer approaches them and thereafter having the information they will be able to sell more. The data, nor my knowledge of Data Scientist would never reveal this use of the information which I created, the domain knowledge did. You can be this representative, who will multiply the created benefit our of Data Science project. 3.5 Data and Computing How is data represented? If you plan on skipping this chapter, because it seems boring, please don’t. It would come to haunt you in your glorious future of Citizen Data Scientist. Data Science is done using computers, thereafter I think it is crucial to understand how your computer sees data. We often times think that computers are smart, while in fact they are quite basic - they understand only two values - 1’s and 0’s. In fact some of the earliest computers were presented data through punchcards. Figure 3.2: Example of how one of the earliest versions of computer from IBM saw data - as punchards with holes (1s) and not-holes (0s). How is it possible that two values are enough to represent anything to the computer? Through converting of numbers into binary: Figure 3.3: Example of how texts and numbers can be encoded into binary. Now what we often times make mistake with is that we overestimate powers of computers and we think that they understand anything - texts, colours, pictures. Well, they don’t. They only really well understand numbers as these can be easily converted into binary form. Thereafter a lot of tasks of Data Scientists revolves around representing data well for the computer. If a Data Sciene project intends to work with images or texts, one of the most crucial tasks is, how do we convert these to numbers, so that they can be represented well for computer in binary forms? I was on a conference where a Scandinavian bank was representing their project of revealing fraud transaction by the means of Data Science. Transaction usually looks as something like following: insert example of a transaction However what a presenter showed was something like following: insert Danske representation of a transaction One of the keys of their project is how the transactions were represented to the computer (and algorithm). We will get more into this later. Important message that I would like you to learn now is that data representation is one of the key components of successful Data Science. How does my PC handle data? As soon as you start a Data Science project, you will be facing hardware limitations. Simple reason for this is that hardware costs. Sure, you can purchase a hardware and have no variable costs, but this hardware will be getting outdated or you might incur opportunity costs. Thereafter, every Data Science project will be optimizing for as little hardware requirements as needed to fulfill particular task. These requirements will be coming from four crucial componenets that a computer has and I think it is hence crucial for you to learn about them now. In order to process data, and run Data Science project, following four components will be needed and this is how I think about them: CPU (processor). RAM Harddisk GPU (graphical processing unit) What is a csv? Now that we know what data is, we also know what we aim to do with it and how will we manage with our computer, it is time to get our hands on some data. The simplest representation of a dataset is through so called “csv” or “comma-separated-values”. In plain form, these might look something like this. Example of one customer data in comma-separated-values form. If you ever worked in Excel, when you are saving document, you can decide to save it as “.csv”. This is by the way a good practice. What is an observation and a feature? Data Science project will be aimed at certain phenomena which is of our interest. This can be our customer base, weather, how certain machine operates, how our colleagues work - anything. In order to apply Data Science to this phenomena, we will need to describe it through observations and features. Observation is some small unit from our phemena such as a day, one customer, one colleague, one moment of machine’s operation. Secondly, feature is characteristic of an observation, such as temperature in a given day, age of one customer, arrival of our colleague to work or speed of machine’s operation. Here is a small table to summarize it: Phenomena Observation Feature weather day/hour/minute humidity customer base one customer age of customer Throughout your entire career, you will be meeting these two concepts of observation and feature. These will vary in forms, but the way we described it now will always hold. Thereafter we can adjust our csv file and we can represent in it one customer with some of his features. As a standard, features are displayed in columns. Example of one customer data in comma-separated-values form. What is a “flat table”? We now leared how to store data about one customer, but we have many customers right? To store data about all of them in reasonable form, we will use a flat table. It is common to store observations in rows. Thereafter, we can just add several rows, representing several customers. Example of one customer data in comma-separated-values form. What is granularity? Even though we can now store customer’s features nicely, we still haven’t won due to the issue of granularity. Some of the features about a customer will have different granularity level and we are unable to store them in the same table in a nice way. What if we wanted to store cups of coffee per day, per customer. With our current knowledge we could do something like this. Example of one customer data in comma-separated-values form. Ugly right? We need to solve this by introducing more tables, whereas these are on different granularity level. We will call this relational database. What is a relational database? Edgar Frank Codd at IBM in 1970 in his paper “A Relational Model of Data for Large Shared Data Banks”, introduced a concept of how the problem above can be solved in a neat way. Luckily for us Data Scientists, Oracle implemented this idea into a working software solution in 1979 (Ever wondered how that company became so successful? This is it.). We create two tables, with different granularities separately. The only extra point which we need to specify is how are these linked together - for example customer name. Example of one customer data in comma-separated-values form. 3.6 Using Data Can we already do Data Science? Now that we constructed ourselves a small relational database with everything that we know about our customer base, are we ready to do Data Science? Yes, so let’s give it a try. We are a coffee seller and we are interested at which age group drinks most of the coffee, so that we can better target our marketing efforts. Do I have to do it in this cumbersome way? Luckily, software can of course help us. Now it is time to turn into learning what software can help is and how. We will need an Integrated-Development-Environment (IDE), engine and maybe a library. What is an IDE, engine and package/library? Even though you are right now at the level of Observer, you might already come in touch with programs or libraries which make Data Science projects. Thereafter, it is important to understand a terminology here. The most obvious are so called IDEs - Integrated Development Environments. These are pretty and easy to work in. Currently, some of the most popular ones among Data Scientists are R-Studio and Jupyter Notebooks. This will be a program which runs on a computer (either a physical one or somewhere on the server), that allows you to utilize some computing engine. There are two very popular engines, which are actually competing with each other - R and Python. Imagine these as a set of functions which are very robust and people like to use them because they are well developed and once a Data Scientist works with them, he can be sure that his work will run. Finally there are libraries. The engines as weel as IDEs are many times licensed under open-source. This makes a lot of sense, because people have a motivation to contribute to these. How will they contribute though? It would be very tricky to integrate work of thousands of people though single software - may it be R or Python. For that, we have libraries. These are way smaller sets of functions, which usually serve a specific purpose. For example, even though R serves Data Scientists, you might decide to write specific library which will be for Data Scientists which are trying to predict zombie apocalypse. You will include a set of specific functions in your library and you may decide to publish it so that other people can use it. What is R? R is engine (or programming language) in which a lot of Data Scientists today work, because of its simplicity and statistical powers. We are able to solve our task with it with several lines of code while we make use of functions which someone else predefined for us. Here is how a solution looks. insert R solution to the problem Let’s talk a little bit about what happens. The function mean comes from a predefined library called base. It’s a bit ugly though, as we did not use any pretty IDE. We can write the same code in R-Studio IDE. This is how it looks. insert R-Studio solution Data Scientists of course like this as you can visually distinguish between various elements. The IDE will have much greater functionalities compared to only working in programming language, but we will get to that later. What is Python? What is Knime? Usually when people talk about Data Scientists, they mention two languages (or frameworks) - R and Python. I however believe that in the future a different approach will dominate if we would like to grow the numbers and powers of Citizen Data Scientists. These are frameworks which does not necessarily require coding, but can do very similar things (at a tradeoff of being slightly less flexible). One such framework is Knime. When thinking about Knime, we can forget our distinction between engine and IDE, here we get both things in one. One important difference to R and Python is that here we are not expected to explicitly write code in order to fulfill our data science task. Let’s give it a try. insert Knime example As you can see we are again doing the same steps, just this time they are represented with visual modules instead of lines of code. Should I use R, Python or Knime? As you are on the level of Observer, you should not choose any. At this point, you should only be able to observe a Data Science project, thereafter when your colleagues say that they work in certain language (most likely one of these three), you know what they talk about and you can see ona very basic level what they are doing. 3.7 Machine Learning What is machine learning? Machine Learning is crucial part of Data Science. Often times when I see courses or training materials, this area is postponed until the student learns a lot of basics, such as data preprocessing. Only then, it is believed, you should be exposed to Machine Learning. I believe the opposite - you should meet it already now (as an Observer), due to two reasons. Firstly, as already mentioned, Machine Learning is something like a hearth of Data Science. Secondly, it will keep your motivation higher - you need to know some cool things that you will be able to do once you finish this book. To explain you properly what Machine Learning is, I need to go into history of computing. Why did computers arise? They arose to simplify or automate tasks done by humans. One such task was to count consensus data (the survey done every 10 years which observes a population of a certain country). The problem in 1920s was that it took over 10 years to count and process the data from a consensus, thereafter making it quite worthless. insert IBM story This was great for a couple of decades. Having computers with certain rules which were doing what otherwise humans could do, was really helping humanity. Rules have problems though - they have hard and strict boundaries. If you want to design a great rule based system, you need to be really careful so that you encode all the patterns which are in the data. Firstly, it is very hard to encode all the patters into our model. Secondly, our population and data change over time. Due to that, even if you manage to encode all of the rules, very quickly will these become outdated simply due to changing nature of our world. If you open the Wikipedia page about Machine Learning, most likely it will be defined through the definition of Arthur Samuel from 1959: Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Here we see the difference - instead of explicit instructions, we are going to seek for patterns and inference in our data. When people present to you Machine Learning, they are often times going to show you pictures like this to look cool. insert some picture of ML buzzwording How do I use data for machine learning? The definition from Arthur Samuel from 1959 has a second part which might anwer just this question: &gt; Machine learning algorithms build a mathematical model based on sample data, known as “training data”, in order to make predictions or decisions without being explicitly programmed to perform the task. We are going to show our machine learning model a setof our data, whic we will call training data. The underlying statistical and/or mathematical model will seek for the data and patterns. Once those are found, the model will be trained Now we can present some new data and ask the model to make a prediction about this new data. Do I need machine learning? These are questions that even experienced Data Scientists often times forget to ask and they end up with what I call “overkill”. If you are considering application of machine learning, the first thing you should ask yourself is whether simpler methods cannot answer your hypothesis. I will recall one Kaggle competition to give you and example (we will get to those later). The competition was regarding game for kids. This game includes various activities such as videos, games and assesments (test). The task for a Machine Learning was to predict how many attempts will it take the kid to pass a certain assessment. There were many people who instantly jumped to Machine Learning and spent hours trying to apply Machine Learning to the problem, while thinking that their solution is pretty decent. All of a sudden a solution came which contained only 10 lines of code. All that the author of this submission did was that he looked at the absolute average of attempts that kids needed to solve a particular assignment. He then made only these predictions and beaten everyone’s Machine Learning solution. He set a new baseline of what people need to beat in order to justify the use of Machine Lerning. Keep this in mind and in the future always consider such a baseline. 3.8 Supervised Learning What is supervised learning? Machine Learning has several subfields to it, on this level (Observer) you are only going to learn the most applicable one - Supervised Learning. Supervised learning deals with issues where we have so called target feature. Do you remember how we discussed observation and its characteristics? Target feature is going to be one characteritic, which is for certain reason of our interest. insert mind map with ??? and supervised learning onl revealed Is target feature always straightforward? No. Often times, finding and clarifying a target feature is going to be among the hardest parts of Supervised Learning. If you remember how I was saying that any lecturer or training who promises to prepare you with exercises for real world - target feature definition is actually one of the reasons. Give examples. What is information leakage? One of the steps we need to ensure is to prevent information leakage. This happens in cases, where our independent features contain some leaked (or spoiler) information about our target feature. Let me give you an example. What is Bias-variance tradeoff? I will start the explanation with a quote from George Box, famous statistician: All models are wrong, but some are useful. As there is a lot of discussion in literature about this famous quote, I guess we can make our own interpretation. When it comes to the second part of this definition that some models are useful, we have already discussed this. You can build a Data Science model for essentially anything, the key is to build a model which will be useful and create benefit. What is more relevant for us now, is the first part of the quote - all models are wrong. I do not want to dig too much into statistics, but what is happening is that you can fit a model which will be perfect with your training data - this is the dataset that your model sees during a training phase. Why do we usually build such a model though? We want to then complete its training phase and whenever we show it new data, which does not contain target feature, the model should make accurate predictions. What George Box hints us is that we will never have a model which will make perfectly correct predictions for both training and testing data. We as Data Scientists are only trying to find some sort of optimal balance between the two. What is regression? It is now time that we expand our Machine Learning mind map. Supervised learning has several subfields, which are influenced by the form of our target feature. In this level, we are going to learn about regression and binary classification. insert mind map with added regression and classification Let’s start with regression. In regression supervised learning our target variable is of numerical form. Here are some examples of numerical target features: Predicting income of a person. Values can be any numeric larger than 0 Euro, such as 580 Euro, 1561.6 Euro. Predicting cooling time of iron during forging process. Values can be any numeric larger than 0 minutes, such as 6 minutes or 257 minutes. Predicting weight change of adults over a period of one year. Values can be any numeric in range -30 Kilograms (Wau) to +30 Kilograms (Uhh). The way Machine Learning model which should be of supervissed regression type is going to be build is exactly as we discussed. We have a set of independent features, and our target feature. How do I know if regression model is good? Let’s say we are a bank and we are predicting what is the income of a customer. Our regression model is trained and we see that the error which it has is on average 15 Euro on every customer prediction. By the first look, this seems great right? Let’s though examine the model deeper and see what is happening. insert plot of income predictions Discuss error. We can see that our model is super accurate for predictions for customers with income 0 Euro (such as students and unemployed people). The error here is close to 0 Euro, so we can be satisfied with our model performance for this set of customers. However, when we look at the right side of the plot, we start to see that our model is very wrong for customers with income over 500 Euro. The errors are often times around 100 Euro. Is this a problem? Data Scientist wouldn’t be sure most likely as for him/her is about fitting of overally good model. This is a point where you as a Citizen Data Scientist with context knowledge need to step in. You know that the customers with 0 Euro income are not that interesting for your bank. The customers which actually matter are the ones with high income, because they are very profitable for you. That is why this regression model is bad, even though from pure statistics it looks good (fact that we on average make an error of 15 Euro). What is binary classification? We can now move to binary classification, which is a supervised model, where target feature can have only two values. These two values can have various forms: True, False Buy, No-Buy Will run through, Will not run through Healthy, Not healthy How do I evaluate if classification model is good? Discuss imbalaned classification issue. Discuss confusion matrix and predicting of what actually matters for us. What is correlation? How does my PC view features? Numerical and categorical features. This needs to come before Supervised Learning as I am explaning classification and regression. Are data scientists evil? Is there a reason why to be scared of these stalkers? Propensity models Can two models and their respective probabilities be really compared? Customer and user You need to distinguish between the two The falacy of absolute numbers For many years what matters a lot is a conversion rate or a success rate. I recommend though to be careful with it. Should I focus on how my code looks? I have a saying: “People have feelings. They feel when you code like sh_t.” "],
["contributor.html", "Chapter 4 Contributor: Part I 4.1 Soft skills 4.2 Cloud 4.3 Databases and Data Lakes 4.4 SQL 4.5 Programming 4.6 Be Curious 4.7 Is Machine Learning Ethical or Racist?", " Chapter 4 Contributor: Part I 4.1 Soft skills How do I approach a task? What are legal regulations such as GDPR? How do I produce a valuable output for my employer? 4.2 Cloud What is cloud? Will I use cloud as a data scientist? How do I work on cloud? 4.3 Databases and Data Lakes What is a database? What is a data lake? 4.4 SQL What is SQL? How do I download entire table using SQL? How do I select columns using SQL? How do I filter rows using SQL? How do I join tables in SQL? 4.5 Programming How do I load data to my IDE? What types of features do we know? What is a function? How do I perform numeric operations in my IDE? 4.6 Be Curious You are already at the level where curiosity will be what keeps you growing. With this chapter we are starting with a new practice that I will be leaving recommendations for additional sources that you can read 4.7 Is Machine Learning Ethical or Racist? "],
["statistician.html", "Chapter 5 Statistician 5.1 Example one 5.2 Example two", " Chapter 5 Statistician Some significant applications are demonstrated in this chapter. 5.1 Example one 5.2 Example two "],
["project-responsible.html", "Chapter 6 Project Responsible", " Chapter 6 Project Responsible We have finished a nice book. "],
["guru.html", "Chapter 7 Guru", " Chapter 7 Guru "]
]
